{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy;Copyright for [Shuang Wu] [2017]<br>\n",
    "Cite from the [coursera] named [Neural network for Machine Learning] from [University of Toronto]<br>\n",
    "Learning notes<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to predict the next word\n",
    "\n",
    "## Simple e.g. of relational info.\n",
    "\n",
    "* ![img41](imgs/img41.jpg)\n",
    "\n",
    "## Another way to express same info.\n",
    "\n",
    "* Make set of propositions using 12 relationships:\n",
    "    * son, daughter, nephew, niece, father, mother, uncle, aunt\n",
    "    * brother, sister, husband, wife\n",
    "* colin has-father james\n",
    "* colin has mother victoria\n",
    "* james has wife victoria\n",
    "    * this follows from the two above\n",
    "    \n",
    "## A relational learning task\n",
    "\n",
    "* Given a large set of triples that come from some family trees, figure out the regularities\n",
    "    * obvious way to express the regularities is as symbolic rules\n",
    "        * (x has-mother y) & (y has-hausband z) => (x has-father z)\n",
    "* Finding symbolic rules involves a difficult search through a very large discrete space of possibilities\n",
    "* Can a NN capture the same knowledge by searching through a continuous space ofweights?\n",
    "\n",
    "## Str. of the NN\n",
    "\n",
    "* ![img42](imgs/img42.jpg)\n",
    "* ![img43](imgs/img43.jpg)\n",
    "\n",
    "## Nets. learns\n",
    "\n",
    "* The 6 hidden units in the bottleneck connected to the input representation of person 1 learn to represent features of people that are useful for predicting the answer\n",
    "    * Nationality, generation, branch of the family tree\n",
    "* These features only useful if the other bottlenecks use similar representations and the central layer learns how features predict other features. E.g.\n",
    "    * input person is of generation 3 and\n",
    "    * relationship requires answer to be one generation up implies\n",
    "    * output person is of generation 2\n",
    "    \n",
    "## Another way to see that it works\n",
    "\n",
    "* Train net on all but 4 of the triples that can be made using the 12 relationships\n",
    "    * needs to sweep through the training set many times adjusting the weights slightly each time\n",
    "* Test it on the 4 held-out cases\n",
    "    * 3/4 correct\n",
    "    * good for 24-way choice\n",
    "    * On much bigger datasets can train on a much smaller fraction of the data\n",
    "    \n",
    "## Large-scale e.g.\n",
    "\n",
    "* For a database of millions of relational facts of the form (A R B)\n",
    "    * Could train a net to discover feature vector representations of the terms that allow the third term to be predicted from the first 2\n",
    "    * then could use the trained net to find very unlikely triples. Good candidates for errors in the database\n",
    "* Instead of predicting the third term, could use all 3 terms as input and predict the probability that the fact is correct\n",
    "    * train net need good source of false facts\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A brief diversion into cognitive science\n",
    "\n",
    "## The family trees e.g. tells us about these concepts\n",
    "\n",
    "* There are been a long debate in cognitive science between two rival theories of what it means to have a concept:\n",
    "    * Feature theory: Concept is a set of semantic features\n",
    "        * Good for explaining similarities b/w concepts\n",
    "        * Convenient: a concept is a vector of feature activities\n",
    "    * Structuralist theory: The meaning of a concept lies in its relationships to other concepts\n",
    "        * So conceptual knowledge is best expressed as a relational graph\n",
    "        * Minsky used the limitations of perceptrons as evidence against feature vectors and in favor of relational graph representations\n",
    "        \n",
    "## Both sides are wrong\n",
    "\n",
    "* These 2 theories need not be rivals. A NN can use vectors of semantic features to implement a relational graph\n",
    "    * In the NN that learns family trees, no **explicit** inference is required to arrive at the intuitively obvious consequences of the facts that have been explicitly learned\n",
    "    * The net can \"intuit\" the answer in a forward pass\n",
    "* We may use explicit rules for conscious, deliberate reasoning, but do a lot of commonsense, analogical reasoning by just \"seeing\" the answer w/ no conscious intervening steps\n",
    "    * Even when using explicit rules, need to just see which rules to apply\n",
    "    \n",
    "## Localist and distributed representations of concepts\n",
    "\n",
    "* Obvious way to implement a relational graph in a NN is to treat a neuron as a node in the graph and a connection as a binary relationship. But this \"localist\" method will not work:\n",
    "    * need many different types of relationship and the connections in a NN do not have discrete labels\n",
    "    * need ternary relationships as well as binary ones. (e.g. A is b/w B and C)\n",
    "* Right way to implement relational knowledge in a NN is still an open issue\n",
    "    * But many neurons are probably used for each concept and each neuron is probably involved in many concepts. This is called a \"distributed representation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another diversion: Softmax output func.\n",
    "\n",
    "## Problems w/ squared error\n",
    "\n",
    "* Drawbacks for squared error measure\n",
    "    * If desired output is 1 and the actual is 0.00000001, there is almost no gradient for a logistic unit to fix up the error\n",
    "    * If trying to assign probabilities to mutually exclusive class labels, know that the outputs should sum to 1, but depriving the network of this knowledge\n",
    "* Different cost func that works better:\n",
    "    * Force the outputs to represent a probability distribution across discrete alternatives\n",
    "    \n",
    "## Softmax\n",
    "\n",
    "$$y_i=\\frac{e^{z_i}}{\\sum_{j\\in group}e^{z_j}}$$\n",
    "\n",
    "$$\\frac{\\partial y_i}{\\partial z_i}=y_i(1-y_i)$$\n",
    "\n",
    "* The output units in a softmax group use a non-local non-linearity:\n",
    "    * ![img44](imgs/img44.jpg)\n",
    "    \n",
    "## Cross-entropy: the right cost func. to use w/ softmax\n",
    "\n",
    "* The right cost func. is the negative log probability of the right answer\n",
    "    * $$C = -\\sum_j t_jlogy_j$$\n",
    "        * $t_j$ is the target value\n",
    "* C has a very big gradient when the target val. is 1 and the output is almost zero\n",
    "    * $$\\frac{\\partial C}{\\partial z_i} = \\sum_j\\frac{\\partial C}{\\partial y_j}\\frac{\\partial y_j}{\\partial z_i}=y_i-t_i$$\n",
    "    * A val. of 0.000001 is much better than 0.000000001\n",
    "    * The steepness of dC/dy exactly balances the flatness of dy/dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neuro-probabilistic language models\n",
    "\n",
    "## A basic problem in speech recognition\n",
    "\n",
    "* Cannot identify phonemes perfectly in noisy speech\n",
    "    * The acoustic input is often ambiguous: there are several different words that fit the acoustic signal equally well\n",
    "* People use their understanding of the meaning of the utterance to hear the right words\n",
    "    * do this unconsciously when wreck a nice beach\n",
    "    * very good at it\n",
    "* this means speech recognizers have to know which words are likely to come next and wihch are not\n",
    "    * Fortunately, words can be predicated quite well w/o full understanding\n",
    "    \n",
    "## The standard \"trigram\" method\n",
    "\n",
    "* Take huge amount of text and count the frequencies of all triples of words\n",
    "* Use these frequencies to make bets on the relative probabilities of words given the previous two words:\n",
    "    * $$\\frac{p(w_3=c|w_2=b,w_1=a)}{p(w_3=d|w_2=b,w_1=a)}=\\frac{count(abc)}{count(abd)}$$\n",
    "* Until very recently this was the state-of-the-art\n",
    "    * Cannot use a much bigger context b/c there are too many possibilities to store and the counts would mostly be zero\n",
    "    * Have to \"back-off\" to digrams when the count for a trigram is too small\n",
    "        * The probability is not zero just b/c the count is zero\n",
    "        \n",
    "## Info. that the trigram model fails to use\n",
    "\n",
    "* Suppose seen the sentence\n",
    "    * \"the cat got squashed in the garden on friday\"\n",
    "* This should help predict words in the sentence\n",
    "    * \"the dog got flattened in the yard on monday\"\n",
    "* A trigam model does not understand the similarities b/w:\n",
    "    * cat/dot\n",
    "    * squashed/flattened\n",
    "    * garden/yard\n",
    "    * friday/monday\n",
    "* To overcome this limitation, need to use the semantic and syntactic features of previous words to predict the features of the next word\n",
    "    * Using a feature representation also allows a context that contains many more previous words\n",
    "    \n",
    "## Bengio's NN for predicting the next word\n",
    "\n",
    "* ![img45](imgs/img45.jpg)\n",
    "\n",
    "## Problem w/ having 100,000 output words\n",
    "\n",
    "* Each unit in the last hidden layer has 100,000 outgoing weights\n",
    "    * So cannot afford to have many hidden units\n",
    "        * Unless have a huge # of training cases\n",
    "    * Could make the last hidden layer small, but then its hard to get the 100,000 probabilities right\n",
    "        * The small probabilities are often relevent\n",
    "* Best way to deal w/ such a large # of outputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ways to deal w/ large # of possible outputs in neuro-probabilistic language models\n",
    "\n",
    "## A serial architecture\n",
    "\n",
    "* ![img46](imgs/img46.jpg)\n",
    "\n",
    "## Learning in the serial architecture\n",
    "\n",
    "* After computing the logit score for each candidate word, use all of the logits in a softmax to get word probabilities\n",
    "* The difference b/w the word probabilities and their target probabilities gives cross-entropy error derivatives\n",
    "    * The derivatives try to raise the score of the correct candidate and lower the scores of its high-scoring rivals\n",
    "* Can save a lot of time if only use a small set of candidates suggested by some other kind of predictor\n",
    "    * e.g., could use the NN to revise the probabilities of the words that the trigram model thinks are likely\n",
    "\n",
    "## Learning to predict the next word by predicting a path through a tree\n",
    "\n",
    "* Arrange all the words in a binary tree w/ words as the leaves\n",
    "* Use the previous context to generate a \"prediction vector\" v.\n",
    "    * Compare v w/ a learned vector, u,  at each node of the tree\n",
    "    * Apply the logistic func. to the scalar product of u and v to predict the probabilities of taking the two branches of the tree\n",
    "    * ![img47](imgs/img47.jpg)\n",
    "\n",
    "## Picture of the learning\n",
    "\n",
    "* ![img48](imgs/img48.jpg)\n",
    "\n",
    "## Convenient decomposition\n",
    "\n",
    "* Maximizing the log probability of picking the target word is equivalent to maximizing the sum of the log probabilities of taking all the branches on the path that leads to the target word\n",
    "    * During learning, only need to consider the nodes on the correct path. This is an exponential win: <font color='orange'> log(N) instead of N </font>\n",
    "    * For each of these nodes, know the correct branch and know the current probability of taking it so can get derivatives for learning both the prediction vector v and that node vector u\n",
    "* Unfortunately, still slow at test time.\n",
    "\n",
    "## A simpler way to learn feature vectors for words\n",
    "\n",
    "* ![img49](imgs/img49.jpg)\n",
    "\n",
    "## Displaying the learned feature vectors in a 2-D map\n",
    "\n",
    "* Can get an idea of the quality of the learned feature vectors by displaying them in a 2-D map\n",
    "    * Display very similar vectors very close to each other\n",
    "    * Use a multi-scale method called \"t-sne\" that also displays similar clusters near each other\n",
    "* The learned feature vectors capture lots of subtle semantic distinctions, just by looking at strings of words.\n",
    "    * No extra supervision is required\n",
    "    * The info. is all in the contexts that the word is used in\n",
    "    * Consider \"She scrammed him w/ the frying pan\"\n",
    "\n",
    "## Part of a 2-D map of the 2500 most common words\n",
    "\n",
    "* ![img50](imgs/img50.jpg)\n",
    "* ![img51](imgs/img51.jpg)\n",
    "* ![img52](imgs/img52.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
