{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy;Copyright for [Shuang Wu] [2017]<br>\n",
    "Cite from the [coursera] named [Neural network for Machine Learning] from [University of Toronto]<br>\n",
    "Learning notes<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to predict the next word\n",
    "\n",
    "## Simple e.g. of relational info.\n",
    "\n",
    "* ![img41](imgs/img41.jpg)\n",
    "\n",
    "## Another way to express same info.\n",
    "\n",
    "* Make set of propositions using 12 relationships:\n",
    "    * son, daughter, nephew, niece, father, mother, uncle, aunt\n",
    "    * brother, sister, husband, wife\n",
    "* colin has-father james\n",
    "* colin has mother victoria\n",
    "* james has wife victoria\n",
    "    * this follows from the two above\n",
    "    \n",
    "## A relational learning task\n",
    "\n",
    "* Given a large set of triples that come from some family trees, figure out the regularities\n",
    "    * obvious way to express the regularities is as symbolic rules\n",
    "        * (x has-mother y) & (y has-hausband z) => (x has-father z)\n",
    "* Finding symbolic rules involves a difficult search through a very large discrete space of possibilities\n",
    "* Can a NN capture the same knowledge by searching through a continuous space ofweights?\n",
    "\n",
    "## Str. of the NN\n",
    "\n",
    "* ![img42](imgs/img42.jpg)\n",
    "* ![img43](imgs/img43.jpg)\n",
    "\n",
    "## Nets. learns\n",
    "\n",
    "* The 6 hidden units in the bottleneck connected to the input representation of person 1 learn to represent features of people that are useful for predicting the answer\n",
    "    * Nationality, generation, branch of the family tree\n",
    "* These features only useful if the other bottlenecks use similar representations and the central layer learns how features predict other features. E.g.\n",
    "    * input person is of generation 3 and\n",
    "    * relationship requires answer to be one generation up implies\n",
    "    * output person is of generation 2\n",
    "    \n",
    "## Another way to see that it works\n",
    "\n",
    "* Train net on all but 4 of the triples that can be made using the 12 relationships\n",
    "    * needs to sweep through the training set many times adjusting the weights slightly each time\n",
    "* Test it on the 4 held-out cases\n",
    "    * 3/4 correct\n",
    "    * good for 24-way choice\n",
    "    * On much bigger datasets can train on a much smaller fraction of the data\n",
    "    \n",
    "## Large-scale e.g.\n",
    "\n",
    "* For a database of millions of relational facts of the form (A R B)\n",
    "    * Could train a net to discover feature vector representations of the terms that allow the third term to be predicted from the first 2\n",
    "    * then could use the trained net to find very unlikely triples. Good candidates for errors in the database\n",
    "* Instead of predicting the third term, could use all 3 terms as input and predict the probability that the fact is correct\n",
    "    * train net need good source of false facts\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A brief diversion into cognitive science\n",
    "\n",
    "## The family trees e.g. tells us about these concepts\n",
    "\n",
    "* There are been a long debate in cognitive science between two rival theories of what it means to have a concept:\n",
    "    * Feature theory: Concept is a set of semantic features\n",
    "        * Good for explaining similarities b/w concepts\n",
    "        * Convenient: a concept is a vector of feature activities\n",
    "    * Structuralist theory: The meaning of a concept lies in its relationships to other concepts\n",
    "        * So conceptual knowledge is best expressed as a relational graph\n",
    "        * Minsky used the limitations of perceptrons as evidence against feature vectors and in favor of relational graph representations\n",
    "        \n",
    "## Both sides are wrong\n",
    "\n",
    "* These 2 theories need not be rivals. A NN can use vectors of semantic features to implement a relational graph\n",
    "    * In the NN that learns family trees, no **explicit** inference is required to arrive at the intuitively obvious consequences of the facts that have been explicitly learned\n",
    "    * The net can \"intuit\" the answer in a forward pass\n",
    "* We may use explicit rules for conscious, deliberate reasoning, but do a lot of commonsense, analogical reasoning by just \"seeing\" the answer w/ no conscious intervening steps\n",
    "    * Even when using explicit rules, need to just see which rules to apply\n",
    "    \n",
    "## Localist and distributed representations of concepts\n",
    "\n",
    "* Obvious way to implement a relational graph in a NN is to treat a neuron as a node in the graph and a connection as a binary relationship. But this \"localist\" method will not work:\n",
    "    * need many different types of relationship and the connections in a NN do not have discrete labels\n",
    "    * need ternary relationships as well as binary ones. (e.g. A is b/w B and C)\n",
    "* Right way to implement relational knowledge in a NN is still an open issue\n",
    "    * But many neurons are probably used for each concept and each neuron is probably involved in many concepts. This is called a \"distributed representation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another diversion: Softmax output func.\n",
    "\n",
    "## Problems w/ squared error\n",
    "\n",
    "* Drawbacks for squared error measure\n",
    "    * If desired output is 1 and the actual is 0.00000001, there is almost no gradient for a logistic unit to fix up the error\n",
    "    * If trying to assign probabilities to mutually exclusive class labels, know that the outputs should sum to 1, but depriving the network of this knowledge\n",
    "* Different cost func that works better:\n",
    "    * Force the outputs to represent a probability distribution across discrete alternatives\n",
    "    \n",
    "## Softmax\n",
    "\n",
    "$$y_i=\\frac{e^{z_i}}{\\sum_{j\\in group}e^{z_j}}$$\n",
    "\n",
    "$$\\frac{\\partial y_i}{\\partial z_i}=y_i(1-y_i)$$\n",
    "\n",
    "* The output units in a softmax group use a non-local non-linearity:\n",
    "    * ![img44](imgs/img44.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
