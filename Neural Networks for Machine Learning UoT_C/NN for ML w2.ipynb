{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy;Copyright for [Shuang Wu] [2017]<br>\n",
    "Cite from the [coursera] named [Neural network for Machine Learning] from [University of Toronto]<br>\n",
    "Learning notes<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the main types of NN arch\n",
    "\n",
    "### Feed-forward NN\n",
    "* Commonest type of NN<br>\n",
    "    * first layer, input<br>\n",
    "    * last layer, output<br>\n",
    "    * more than one hidden layer, \"deep\" NN<br>\n",
    "* Compute series of transformations that change the similarities b/w cases<br>\n",
    "    * activities of the neurons are non-linear function of the activities in the layer below\n",
    "![img14](imgs/img14.jpg)\n",
    "\n",
    "### Recurrent networks\n",
    "* directed cycles in their connection graph\n",
    "    * somtimes can get back to where you started by folloing the arrows\n",
    "* Can have complicated dynamics and can make RNN difficult to train\n",
    "    * lot of interest ar present in finding efficient ways of training RNN\n",
    "* More biologically realistic\n",
    "* RNN w/ mult. hidden layers are a special case that has some of the hidden to hidden connections missing.\n",
    "![img15](imgs/img15.jpg)\n",
    "\n",
    "### RNN for modeling sequences\n",
    "* Natural way to model sequential data:\n",
    "    * Equivalent to very deep nets w/ 1 hidden layer per time slice.\n",
    "    * Use the same weights at every time slice and get input at every time slice\n",
    "* able to remember info. in hidden state for long time\n",
    "    * hard to train use this potential\n",
    "![img16](imgs/img16.jpg)\n",
    "\n",
    "### E.g of RNN\n",
    "* 2011, train a special type of RNN to predict the next character in a sequence\n",
    "* after training for a long time on a string of half a billion characters from English Wiki, got it to generate new text.\n",
    "    * Generating by predicting the prob. distribution for the next character and then sampling a character from this dist.\n",
    "    * Text it generates e.g.\n",
    "    ![img17](imgs/img17.jpg)\n",
    "    \n",
    "### Symmetrically connected networks\n",
    "* Like RNN, but connections between units are symmetrical (same weight in both directions)\n",
    "    * much easier to analyze than RNN\n",
    "    * more restricted in what they can do, b/c obey an anergy function\n",
    "        * can not model cycles\n",
    "* Symmetrically connected nets w/o hidden units: \"Hopfield nets\"\n",
    "* SCN w/ hidden units: \"Boltzmann machines\"\n",
    "    * much more powerful than Hopfield nets\n",
    "    * less powerful than RNN\n",
    "    * beautifully simple learnign algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1st generation of NN\n",
    "\n",
    "### Standard paradigm for statistical pattern recognition\n",
    "1. Convert raw input vec. into a vec. of feature actions\n",
    "    * hand-written programs based on common-sense to define the features\n",
    "2. Learn: weight each feature actions to get a single scalar quantity\n",
    "3. If above some threshold, input vec. is a positive example of the target\n",
    "\n",
    "### The standard perceptron architecture\n",
    "![img18](imgs/img18.jpg)\n",
    "\n",
    "### History of perceptrons\n",
    "* Popularised in 1960's\n",
    "    * appeared have very powerful learning algorithm.\n",
    "    * lots of grand claims were made for what they could learn to do\n",
    "* 1969,  they could do and showed their limitations\n",
    "    * Many people thought these limitations applied to all NN models\n",
    "* Perceptron learning procedure still widely used today for tasks w/ enormous feature vectors that contain many millions of features.\n",
    "\n",
    "### Binary threshold neurons (decision units)\n",
    "* 1943 (McCulloch-Pitts)\n",
    "    * 1st compute a weighted sum of the inputs from other neurons (plus a bias)\n",
    "    * output 1 if weighted sum exceeds zero\n",
    "$$z = b + \\sum_i x_i w_i$$\n",
    "$$y = \\begin{cases} 1 &\\mbox{if}\\quad  z \\geq 0\\\\ 0 &\\mbox{o.w.}\\end{cases}$$\n",
    "![img19](imgs/img19.jpg)\n",
    "\n",
    "### How to learn biases\n",
    "* A threshold equivalent to having a negative bias\n",
    "* Can avoid having to figure out a separate learning rule for the bias by using:\n",
    "    * A bias is exactly equivalent to a weight on an extra input line that always has an activity of 1\n",
    "    * Can now learn a bias as if it were a weight\n",
    "![img20](imgs/img20.jpg)\n",
    "\n",
    "### Perceptron convergence procedure: Training binary output neurons as classifiers\n",
    "* Add extra component w/ value 1 to each input var. The 'bias' weight on this component is minus the threshold. Now can ignore the threshold.\n",
    "* Pick training case using any polict that ensures every training case will keep getting picked.\n",
    "    * If output correct, leave weights alone\n",
    "    * output incorrectly output a 0, add input vector to weight vector\n",
    "    * incorrectly output a 1, subtract the input vetor from the weight vector\n",
    "* Guaranteed to find a set of weights that gets the right answer for all the training cases <font color=\"red\">if such set exists</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Geometrical view of perceptrons\n",
    "### Weight-space\n",
    "* 1-D per weight\n",
    "* A point in the space represents a particular setting of all the weights\n",
    "* Assume we have eliminated the threshold, each training case can be represented as a hyperplane through the origin\n",
    "    * Weights must lie on one side of this hyper-plance to get the correcta ans.\n",
    "* Each training case defines a plane (the black line)\n",
    "    * plane goes through the origin and is perpendicular to the input vector\n",
    "    * on one side of the plance the output is wrong b/c scalar product of the weight vector w/ the input vector has the wrong sign.\n",
    "![img21](imgs/img21.jpg)\n",
    "![img22](imgs/img22.jpg)\n",
    "\n",
    "### The cone of feasible solutions\n",
    "* To ge all training right, need find a point on the right side of all the planes\n",
    "    * may not be any such point\n",
    "* If  any, they lie in a hyper-cone w/ its apex at the origin\n",
    "    * average of 2 good weight vectors is a good weight vector\n",
    "        * problem is convex\n",
    "![img23](imgs/img23.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why the learning works\n",
    "### 1st attempt\n",
    "* the $^2_a+d^2_b$ b/w any feasible weight vector and the current weight vector\n",
    "    * every time the perceptron makes a mistake, the learning algorithm moves the current weight vector closer to all feasible weight vectors\n",
    "![img24](imgs/img24.jpg)\n",
    "\n",
    "* Consider \"generously feasible\" weight vectors lie within the feasible region by a margin at least as great as the length of the input vector that defines each constraint plane\n",
    "    * every time the perceptron makes mistake, the squared distance to all of these generously feasible weight vectors is always decreas by at least the squared length of the update vector\n",
    "![img25](imgs/img25.jpg)\n",
    "\n",
    "### Informal sketh of prood of convergence\n",
    "* Each time the perceptron makes a mistake, current weight vector moves to decrease tis squared distance from every weight vector in \"generously feasible\" region\n",
    "* Squared distance decreases by at least the squared length of the input vector.\n",
    "* after finite # of mistakes, weight lie in the feasible region if region exists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What perceptrons can't do\n",
    "### Limitations of perceptrons\n",
    "* if choose features by hand and use enough features, can do almost anything\n",
    "    * for binary input, can have a separate feature unit for each of the exponentially many binary vec. and we can make any possible discrimination on binary input vectors.\n",
    "        * this type of table look-up won't generalize\n",
    "* Once the hand-coded features have been determined, there are strong limitations on what a perceptron can learn.\n",
    "\n",
    "### Binary threshold neurons cannot do\n",
    "* two single bit features are the same\n",
    "    * Positive case(same): $(1,1)\\rightarrow 1; \\quad (0,0)\\rightarrow 1$\n",
    "    * Negative case(different): $(1,0)\\rightarrow 0; \\quad (0,1)\\rightarrow 0$\n",
    "* 4 inpput-output pairs give 4 inequalities that impossible to satisfy:\n",
    "    $$w_1+w_2\\geq\\theta, \\quad 0\\geq\\theta$$\n",
    "    $$w_1 <\\theta, \\quad w_2<\\theta$$\n",
    "![img26](imgs/img26.jpg)\n",
    "\n",
    "### Geometric view\n",
    "* Imagine \"data-space\" in which the axes correspond to components of an input vector\n",
    "    * each input is a point in this space\n",
    "    * a weight vector defines a plane in data-space\n",
    "    * weight plane is perpendicular to the w8 vec and misses the origin by a distance equal to the t.s.\n",
    "![img27](imgs/img27.jpg)\n",
    "    \n",
    "### Discriminating simple patterns under translation w/ wrap-around\n",
    "* Use pixels as features\n",
    "* can discriminate b/w diff. patterns that have the same # of pixels?\n",
    "    * not if patterns can translate w/ wrap-around\n",
    "![img28](imgs/img28.jpg)\n",
    "\n",
    "### Sketch of a proof that a binary decision unit cannot discriminate patterns w/ the same # of on pixels (assuming translation w/ wraparound\n",
    "* Pattern A, use training cases in all possible translations\n",
    "    * each pixel will be activated by 4 diff. translations of pattern A\n",
    "    * total input received by the decision unit over all these patterns will be four times the sum of all the weights\n",
    "* Pattern B, use traning cases in all possible trnaslations\n",
    "    * each pixel will be activated by 4 diff. translations of pattern B\n",
    "    * total input received by the decision unit over all these patterns will be four times the sum of all the weights\n",
    "* To discriminate correctly, every single case of A must provide more input to the decision unit than every single case of pattern B\n",
    "    * imposiible if sums over cases are same\n",
    "    \n",
    "### Why result devastating for perceptrons\n",
    "* Whole point of pattern recognition is to recognize patterns despite transformations like translation\n",
    "* Minsky and Papert's \"Group Invariance theorem\" says that the part of a perceptron that learns cannot learn to do this if the trans. form a group\n",
    "    * translations w/ wrap-around form a group\n",
    "* To deal w/ such tran., a perceptron needs to use multiple feature units to recognize trans. of informative sub-patterns\n",
    "    * tricky part of pattern recognition must be solved by the hand-coded feature detectors, not learning procedure\n",
    "    \n",
    "### Learning w/ hidden units\n",
    "* Networks w/o hidden units limited in the input-output mappings they can learn to model\n",
    "    * More layers of linear units do not help, still linear\n",
    "    * fixed output non-linearities not enough\n",
    "* need multiple layers of adaptive, non-linear hidden units. but how to train?\n",
    "    * need efficient way of adapting all the weight, not just the last layer\n",
    "    * learning the weights going into hidden units is equivalent to learning features\n",
    "    * difficult because nobody is telling us directly what the hidden units should do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
