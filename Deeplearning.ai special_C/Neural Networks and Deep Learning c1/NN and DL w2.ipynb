{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy;Copyright for [Shuang Wu] [2017]<br>\n",
    "Cite from the [coursera] named [Neural network and Machine Learning] from [deeplearning.ai]<br>\n",
    "Learning notes<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression as a NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "* Binary Classification\n",
    "    * lagel the img is cat (1) vs non-cat (0) (y)\n",
    "        * img is in as red, green, blue intensities\n",
    "        * each pixel is a value, e.g. $64*64$\n",
    "        * x is just list all these pixeles in a single col.\n",
    "        * x can also be $64*64*3=12288$\n",
    "        * $n_x=12288$\n",
    "        * want to predict $x\\rightarrow y$\n",
    "        ![img12](imgs/img12.jpg)\n",
    "* Notation\n",
    "    * $(x,y)$ single example\n",
    "        * $x\\in\\mathbb{R}^{n_x}$\n",
    "        * $y\\in{0,1}$\n",
    "    * m training example: {($x^1$, $y^1$), ($x^2$, $y^2$),$\\cdots$,($x^m$, $y^m$)}\n",
    "        * $m=m_{train}$\n",
    "        * $m=m_{test}$\n",
    "    * X = $[\\vec{x^1}, \\vec{x^2}, \\cdots, \\vec{x^m},]$\n",
    "        * $X\\in\\mathbb{R}^{n_x*m}$\n",
    "        * X.shape=($n_x$,m)\n",
    "        ![img13](imgs/img13.jpg)\n",
    "    * Y = [$y^1$, $y^2$, $\\cdots$, $y^m$]\n",
    "        * $Y\\in\\mathbb{R}^{1*m}$\n",
    "        * Y.shape = (1,m)\n",
    "        ![img14](imgs/img14.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Reg.\n",
    "* Logistic regression is a learning algorithm used in supervised learning problem\n",
    "    * the output y are all 0 or 1\n",
    "    * goal is to minimize the error b/w predictions and training data\n",
    "* E.g.: Cat vs No-cat\n",
    "    * Given an image represented by feature vector $x$\n",
    "    * Probability of cat in img:\n",
    "        * $Given\\quad x, \\hat{y}=P(y=1|x)\\quad, where\\quad0\\leq\\hat{y}\\leq1$\n",
    "    * Parameters in the logistic reg:\n",
    "        * input fectures vecotr: $x\\in\\mathbb{R}^{n_x}$\n",
    "        * training label: y\n",
    "        * weights: $w\\in\\mathbb{R}^{n_x}$\n",
    "        * threshold: $b\\in\\mathbb{R}$\n",
    "        * output $\\hat{y}=\\sigma(w^Tx+b) = \\sigma(z)$\n",
    "        * sigmoid function: $s=\\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
    "        ![img15](imgs/img15.jpg)\n",
    "        * $\\sigma(w^Tx+b)$ is a linear function $(ax+b)$, but we are looking for prob. constraint b/w [0,1], so use sigmoid func.\n",
    "        * Obs. from this graph\n",
    "            * bounded b/w [0,1]\n",
    "            * if z large positive, $\\sigma(z)=1$\n",
    "            * if z small negative, $\\sigma(z)=0$\n",
    "            * if $z=0$, $\\sigma(z)=0.5$\n",
    "* It's easy to use b and w separately instead of using the $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Reg. Cost function\n",
    "\n",
    "* Loss (error) function:\n",
    "    * Measure the discrepancy b/t predication ($\\hat{y}^{(i)}$) and desired output ($y^{(i)}$)\n",
    "    * computes the error for single training example\n",
    "    * $L(\\hat{y}^{(i)}, y^{(i)}) = \\frac{1}{2}(\\hat{y}^{(i)}-y^{(i)})^2$\n",
    "        * not convex so do not use this, because the GD will not find the global minimization\n",
    "    * $L(\\hat{y}^{(i)}, y^{(i)}) = -(y^{(i)}log(\\hat{y}^{(i)}) + (1-y^{(i)})log(1-\\hat{y}^{(i)}))$<br>\n",
    "        * this will make the convex\n",
    "    <br>\n",
    "        * If $y^{(i)}=1$: $L(\\hat{y}^{(i)}, y^{(i)})=-log(\\hat{y}^{(i)})$ where $log(\\hat{y}^{(i)})$ and $\\hat{y}^{(i)}$ should be close to 1\n",
    "        * If $y^{(i)}=0$: $L(\\hat{y}^{(i)}, y^{(i)})=-log(1-\\hat{y}^{(i)})$ where $log(1-\\hat{y}^{(i)})$ and $\\hat{y}^{(i)}$ should be close to 0<br>\n",
    "        <br>\n",
    "* Cost function\n",
    "    * To train para. w and b\n",
    "    * The average of the loss func. of the entire training set\n",
    "    * want the para. w and b that <font color='red'>minimize</font> the overall cost func.\n",
    "    $$J(w,b)=\\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)}, y^{(i)})=-\\frac{1}{m}\\sum^m_{i=1}[(y^{(i)}log(\\hat{y}^{(i)}) + (1-y^{(i)})log(1-\\hat{y}^{(i)}))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "* GD\n",
    "    * Convex\n",
    "    ![img16](imgs/img16.jpg)\n",
    "    * non-convec\n",
    "    ![img17](imgs/img17.jpg)\n",
    "    ![img18](imgs/img18.jpg)\n",
    "    * initialize like the top red dot\n",
    "    * then take step down hill in the direction\n",
    "    * repeat the process until converge tor close to the global optimal\n",
    "    * Process\n",
    "        * repeat do {w:= w-$\\alpha\\frac{dJ(w)}{dw}$}\n",
    "            * $\\alpha$ is the learning rate\n",
    "            * $w:=w-\\alpha dw$\n",
    "            * the derivative is the slope\n",
    "        ![img18](imgs/img19.jpg)\n",
    "        * $J(w, b)$ \n",
    "            * $w:=w-\\alpha\\frac{\\partial J(w,b)}{\\partial w}$\n",
    "                * $w:=w-\\alpha\\partial w$\n",
    "            * $b:=b-\\alpha\\frac{\\partial J(w,b)}{\\partial b}$\n",
    "                * $b:=b-\\alpha\\partial b$\n",
    "            * use $\\partial$ when the function have at least two variable\n",
    "        * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives\n",
    "\n",
    "* Intuition\n",
    "    * $f(a)=3a$\n",
    "    ![img20](imgs/img20.jpg)\n",
    "        * a=2, f(a)=6\n",
    "            * a=2.001, f(a)=6.003\n",
    "            * increase the $a$ by 0.001, same as increase the $f(a)$ by 0.003\n",
    "            * slope (derivative) of $f(a)$ at $a=2$ is 3\n",
    "            * slope os the height divided by the width\n",
    "        * a=5, f(a)=15\n",
    "            * a=5.001, f(a)=15.003\n",
    "            * slope (derivative) of $f(a)$ at $a=5$ is 3\n",
    "        * $\\frac{df(a)}{da}=3=\\frac{d}{da}f(a)$\n",
    "        * stright line have the same slope everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Derivatives\n",
    "\n",
    "* Intuition\n",
    "    * $f(a)=a^2$\n",
    "    ![img21](imgs/img21.jpg)\n",
    "        * a=2, f(a)=4\n",
    "            * a = 2.001, $f(a)\\approx 4.004$\n",
    "            * slope (derivative) of f(a) at a=2 is 4\n",
    "            * $\\frac{d}{da}f(a)=4$ when a =2\n",
    "        * a=5, f(a)=25\n",
    "            * a =5.001, $f(a)\\approx 25.010$\n",
    "            * slope (derivative) of f(a) at a=5 is 10\n",
    "            * $\\frac{d}{da}f(a)=10$ when a =5\n",
    "        * $\\frac{d}{da}f(a)=\\frac{d}{da}a^2=2a$\n",
    "* more e.g.\n",
    "    * $f(a)=a^3$\n",
    "        * $\\frac{d}{da}f(a)=3a^2$\n",
    "    * $f(a)=log(a)$\n",
    "        * $\\frac{d}{da}f(a)=\\frac{1}{a}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation graph\n",
    "\n",
    "* J(a,b,c) = 3(a + bc)\n",
    "    * u=bc\n",
    "    * v =a + u\n",
    "    * J = 3*v\n",
    "    ![img22](imgs/img22.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives w/ a computation graph\n",
    "\n",
    "* one step back to calculate the derivative\n",
    "* Chain rule\n",
    "    * $\\frac{dJ}{da}= \\frac{dJ}{dv}\\frac{dv}{da}$\n",
    "    ![img23](imgs/img23.jpg)\n",
    "* Final Output Var.\n",
    "    * $\\frac{d FOV}{d Var}$\n",
    "        * \"d var\"\n",
    "    ![img24](imgs/img24.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression Gradient Descent\n",
    "\n",
    "* ![img25](imgs/img25.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GD on m Examples\n",
    "\n",
    "* use for loop and then divided by m\n",
    "* ![img26](imgs/img26.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "* Vectorization\n",
    "    * $z=w^Tx+b$\n",
    "        * w and x are both vectors $\\in\\mathbb{R}^{n_x}$\n",
    "* GPU and CPU\n",
    "    * SIMD - single instantian multiple data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# non-vec cal\n",
    "z=0\n",
    "for i in range(n):\n",
    "    z+= w[i]*x[i]\n",
    "    z+=b\n",
    "    \n",
    "## vec cal\n",
    "z = np.dot(w,x) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250233.868624\n",
      "Vectorized version: 3.0002593994140625ms\n",
      "250233.868624\n",
      "For loop: 743.9999580383301ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "a=np.array([1,2,3,4])\n",
    "\n",
    "a=np.random.rand(1000000)\n",
    "b=np.random.rand(1000000)\n",
    "\n",
    "tic = time.time()\n",
    "c = np.dot(a,b)\n",
    "toc=time.time()\n",
    "\n",
    "print(c)\n",
    "print(\"Vectorized version: \" + str(1000*(toc-tic))+\"ms\")\n",
    "\n",
    "c=0\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c += a[i]*b[i]\n",
    "toc = time.time()\n",
    "\n",
    "print(c)\n",
    "print(\"For loop: \" + str(1000*(toc-tic))+\"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vec. logsitic regression gradient output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on python/numpy vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick tour of Jupyter/ipy nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanationof logistic regression cost func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
