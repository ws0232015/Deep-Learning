{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy;Copyright 2017 Shuang Wu<br>\n",
    "cite from the Neural Networks and Deep Learning book by Michael Nielsen http://neuralnetworksanddeeplearning.com <br>\n",
    "Learning notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NN & DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CH 1\n",
    "## Using NN to recognize handwritten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Human vision involves entire series of visual cortices $V1$, $V2$, $\\cdots$ $V5$, doing complex image processing. We are stupendously, astoundingly good at making sense of what our eyes show us, but done unconscilously.<br>\n",
    "![handW1](/notebooks/imgs/handW1.jpg)\n",
    "Computer recognize is more difficult. The shape of \"9\" which include a loop and a top is hard to transfer to algorithm. When try use this rule, always hopeless.<br>\n",
    "\n",
    "Idea for NN is take large number of handwritten digits, training examples, and then develop a system learn from training examples. NN use examples automatically infer rules to do the recogniz. Increase the # of examples will increase the accuracy.<br>\n",
    "![handW2](/notebooks/imgs/handW2.jpg)\n",
    "NN are used by banks to process cheques and post offices to recognize addresses.          \n",
    "Handwritting reognition is a good prototype example for learning NN in general. And can also apply to speech, natural language processing, etc..<br>\n",
    "\n",
    "2 Important types of artificial neuron:\n",
    "  1. Perceptron\n",
    "  2. Sigmoid neuron<br>\n",
    "  \n",
    "NN Standard learning algorithm: stochastic gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percetron\n",
    "Perceptron takes binary inputs and produces a single binary output:<br>\n",
    "![percep1](/notebooks/imgs/percep1.jpg)\n",
    "The neuron's output, 0 or 1, is determined by the weighted sum $Sigma_j w_jx_j$ is less than or greater than some threshold value, 0 or 1.<br>\n",
    "![percep2](/notebooks/imgs/percep2.jpg)\n",
    "By varying the weights and the threshold, we can get diff. models of decision-making.<br>   \n",
    "Perceptron isn't a complete model of human decision-making, but a complex networks of perceptrons could make quite subtle decisions:<br>\n",
    "![percep3](/notebooks/imgs/percep3.jpg)\n",
    "The 1st Layer making three decisions. 2nd making four decisions by weighing up the results from the 1st layer. 2nd will make more complex and abstract decisions than 1st layer. 3rd layer make more complex decision than 2nd. Many-layer network of percetrons can engage in sophisticated decision making.<br>\n",
    "<br><br>\n",
    "<strong>Math</strong>:    \n",
    "$w\\cdot{x}\\equiv\\Sigma_j w_jx_j$, $w$ and $x$ are vectors for weights and inputs. $b\\equiv\\text{-threshold}$, $b$ is perceptron's bias.<br>\n",
    "\n",
    "$$\\text{output} = \\begin{cases} 0 & \\quad \\text{if}\\quad w\\cdot x+b\\leq 0\\\\\n",
    "                                              1 & \\quad \\text{if}\\quad w\\cdot x+b> 0\\\\ \\end{cases}$$  \n",
    "\n",
    "NAND, not AND:<br>\n",
    "A perceptron with weights -2, -2 and a bias 3:<br>\n",
    "  1. when input 00, (-2) * 0 + (-2) * 0 + 3 = 3, positive 1. not (F&F) = T\n",
    "  2. when input 10 or 01, (-2) * 1 + (-2) * 0 + 3 = 1, positive 1. not (F&T) = T\n",
    "  3. when input 11, (-2) * 1 + (-2) * 0 + 3 = -1, negative 0. not (T&T) = F\n",
    "Can use perceptron to compute simple logical functions. Can conput any logical function. NAND gates are universal for computtion, same as perceptrons.<br>\n",
    "\n",
    "NAND to bitwise sum:<br>\n",
    "![percep4](/notebooks/imgs/percep4.jpg)<br>\n",
    "![percep5](/notebooks/imgs/percep5.jpg)<br>\n",
    "![percep6](/notebooks/imgs/percep6.jpg)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid neurons (Sigmoid also named as logistic)\n",
    "\n",
    "For the learning algorithms, we want small change to the weights or boas will also have small change in output. Than we can modify to let the network behave more in manner we want. BUT not when include single perceptron in network. Small change may cause result flip and cause output complete change.<br>\n",
    "\n",
    "Sigmoid neuron, artificial neuron to solve the problem. Similar to perceptrons but small change gain small change, the crucial fact allow network of sigmoid neuron to learn.<br>\n",
    "![sigmoid1](/notebooks/imgs/sigmoid1.jpg)<br>\n",
    "\n",
    "Like perceptron, sigmoid neuron has inputs. The values can take not only 0, 1 but also between 0 and 1, like 0.6. Have the weights and bias like before. Output is not 0 or 1, it's $\\sigma (w{\\cdot}x+b)$ where $\\sigma\\equiv\\frac{1}{1+e^{-z}}$<br>\n",
    "\n",
    "Sigmoid neuron with input, weights and bias is:<br>\n",
    "$$\\frac{1}{1+exp(-\\Sigma_j w_jx_j-b)}$$\n",
    "\n",
    "Sigmoid function:<br>\n",
    "![sigmoid1](/notebooks/imgs/sigmoid2.jpg)<br>\n",
    "When $z$ goes to $+\\infty$, value goes to 1, vice versa. Smoothed out version of a step func.<br>\n",
    "And we can calculate the how the change of weights and change of bias will affect the change of output:<br>\n",
    "\n",
    "The output of sig. neuron with inputs, weights and bias is:<br>    \n",
    "$$\\Delta\\text{output}\\approx\\sum_{j}\\frac{\\partial\\text{output}}{\\partial{w_j}}\\Delta{w_j}+\\frac{\\partial\\text{output}}{\\partial{b}}\\Delta{b}$$ \n",
    "\n",
    "The equation says: $\\Delta\\text{output}$ is a <i>linear function</i> of the changes $\\Delta w_j$ and $\\Delta b$ in the weights and bias. Much easier to figure out how changing the weights and biases will change the output.<br>\n",
    "\n",
    "$\\sigma$ use widely because the pretty derivative properties, commonly-used in work on neural nets.<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The architeture of neural networks\n",
    "\n",
    "![arch1](/notebooks/imgs/arch1.jpg)<br>\n",
    "From left to right: input layer with input neurons. Hidden, not input or output, layer with hidden neurons. Output layer with output neuron. Hidden can have single or multiple layer.<br>\n",
    "\n",
    "Sometimes multiple layer networks called multilayer perceptrons, <strong>MLP</strong>s, even made by sigmoid.<br>\n",
    "\n",
    "<i>Feedforward neural networks</i>, the output from one layer is used as input to the next layer. No loops in the network for fed forward.<br>\n",
    "\n",
    "<i>Recurrent neural networks</i>, have the feedback loops, use the output for the input. Learning algorithms less powerful, but much closer in spirit to how our brains work than fed f. Can solve important problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simplle network to classify handwritten\n",
    "\n",
    "Split into 2 problem:<br>\n",
    "1. Breaking an image with digits to separate images, each containing single digit.<br>\n",
    "![handw3](/notebooks/imgs/handw3.jpg)<br>\n",
    "2. Classify each individual digit, focus on this problem.<br>\n",
    "\n",
    "To solve the recognizing individual handwritten digits problem, using 3-layer neural network:<br>\n",
    "![handw4](/notebooks/imgs/handw4.jpg)<br>\n",
    "The input layer contains $784=28*28$ neurons because the input image is 28 by 28 pixel. The pixels are greyscale with 0 for white and 1 for black, after normalize by divided 255. The second layer is hidden layer with value n neurous. The output layer have 10 neurons, with the value from 0 to 1. The sum of all these 10 values will be 1 and the largest one from the 10 neurons decided the image's digit, e.g. first neuron is largest means input number is 0.<br>\n",
    "\n",
    "4 output neurons with binary 0 or 1 is already enough to have all 10 situation, but the 10 output neurons have the best prediction. If use 4 outputs, the first neuron would be trying to decide the most significant bit of the digit was, which is not realistic. Just heuristic, can try whatever you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with gradient descent\n",
    "\n",
    "First, need data set to learn from, training data set (MNIST).<br>\n",
    "The data set has 2 parts. 1st, 60,000 images with labels as training data. 2nd, 10,000 images with labels as test data. All $28*28$ grescale images. Test data written by different people compare to the training data.<br>\n",
    "\n",
    "$x$ denote training input, 784-dim vector. Output is $y=y(x)$, 10-dim col. vector. We want find weights and biases for $\\hat{y}\\approx y=y(x)$, the $\\hat{y}$ is the estimator.<br>\n",
    "\n",
    "To quantify how well we're achieving, define <i>cost function</i>, or loss, objective function: <br>\n",
    "$$C(w, b)\\equiv\\frac{1}{2n}\\sum_{x} \\|y(x)-a\\|^2$$\n",
    "$w$ is a vector contain all weights, $b$ is a vector contain all biases, $n$ is the # of total trining inputs. $a$ is the output denpends on $x$, $w$, and $b$. $\\|v\\|$ is the length function for vector $v$. C is the <i>quadratic</i> cost function.<br>\n",
    "Cost function is non-negative and becomes small, approx to 0, when $y(x)$ approx equal to output, $a$, for all training inputs, $x$.<br>\n",
    "\n",
    "The goal is to minimize the cost function $C(w, b)$, we use the algorithm known as <i>gradient descent</i>. <br>\n",
    "We do not dirrectly look the number of accuracy becuase we need to find smooth relation between the outputs and the weights and biases. Thats why we look at the quadratic cost function, easy to make small changes in weights and biases to inprove the cost.<br>\n",
    "<br>\n",
    "\n",
    "<strong>Gradient descent</strong>\n",
    "$$\\min_{C(v)}$$\n",
    "for $v = v_1, v_2,\\cdots$. Suppose when consider of just 2 variables $v_1$ and $v_2$:\n",
    "![gs1](/notebooks/imgs/gs1.jpg)<br>\n",
    "It's easy to found the global minimum from the above plot.<br>\n",
    "One way to solve is to use calculus to find the minimum analytically. Comput derivatives and then try using them to find places where C is an extremum. But work when C is a function of just one or few variables. <strong>Not for Neural networks.</strong><br>\n",
    "\n",
    "Move the ball a small amount $\\Delta v_1$ in the $v_1$ direction, and a small amount $\\Delta v_2$ in the $v_2$ direction. $C$ changes as follows:<br>\n",
    "$$\\Delta C \\approx\\frac{\\partial C}{\\partial v_1}\\Delta v_1 + \\frac{\\partial C}{\\partial v_2}\\Delta v_2$$<br>\n",
    "Going to choose  $\\Delta v_1$ and $\\Delta v_2$ so make $$\\Delta C$$ negative. Define $\\Delta v$ to be the vector of changes in $v$, define the gradient of $C$:<br>\n",
    "$$\\nabla{C}\\equiv(\\frac{\\partial C}{\\partial v_1},\\frac{\\partial C}{\\partial v_2})^T$$\n",
    "$$\\Delta C \\approx \\nabla C\\cdot\\Delta v$$<br>\n",
    "\n",
    "Choose of $\\Delta v$ to make the $\\Delta C$ negative, suppose we choose:<br>\n",
    "$$\\Delta v = -\\eta\\nabla C$$<br>\n",
    "where $\\eta$ is a small, positive parameter, known as <i>learning rate</i>, then the equation change to:<br>\n",
    "$$\\Delta C \\approx -\\eta\\nabla C\\cdot\\nabla C = -\\eta\\|\\nabla C\\|^2$$<br>\n",
    "because $\\|\\nabla C\\|^2\\geq 0 $, so $\\Delta C \\leq 0$, C will always decreaase.<br>\n",
    "\n",
    "This is what we look for and will use this to calculate $\\Delta v$ and move the position $v$ by that amount:<br>\n",
    "$$v \\rightarrow v' = v - \\eta\\nabla C$$<br>\n",
    "We use this rule to keep updating the value to decreasing $C$ until reach the \"global minimum\", can visualize like this:<br>\n",
    "![gs2](/notebooks/imgs/gs2.jpg)<br>\n",
    "\n",
    "To make the dradient descent work, need to choose the learning rate $\\eta$ to be small enough. If not, may end up with $\\Delta C > 0$, but if learning rate to small, it will take a long time to find the minimum. Same thing when more than 2 variables.<br>\n",
    "\n",
    "Gradient descent with the neuron networks. The updating rule with weights and biases are:<br>\n",
    "$$w_k \\rightarrow w'_k = w'_k-\\eta\\frac{\\partial C}{\\partial w_k}$$<br>\n",
    "$$b_l \\rightarrow b'_l = b'_l-\\eta\\frac{\\partial C}{\\partial b_l}$$<br>\n",
    "By updating these, reach the minimum of the cost function. But when the # if training inputs is very large this will take long time, the <i>stochastic gradient descent</i> can used to speed up. The idea is to estimate the gradient $\\nabla C$ by computing $\\nabla C_x$ for a small sample of randomly chosen training inputs.<br>\n",
    "\n",
    "<strong>Stochastic gradient descent</strong><br>\n",
    "selec m of randomly chosen training inputs. Provided the sample size m is large enough, we want the gradient value roughly equl:<br>\n",
    "$$\\frac{\\sum_{j=1}^{m}\\nabla C_{x_{j}}}{m}\\approx \\frac{\\sum_{x}\\nabla C_{x}}{n} = \\nabla C$$<br>\n",
    "Stochastic gradient descent works by picking out a randomly chose nmini-batch of training inputs, and training with those:<br>\n",
    "$$w_k \\rightarrow w'_k = w_k-\\frac{\\eta}{m}\\sum_{j}\\frac{\\partial C_{x_j}}{\\partial w_k}$$<br>\n",
    "$$b_l \\rightarrow b'_l = b_l-\\frac{\\eta}{m}\\sum_{j}\\frac{\\partial C_{x_j}}{\\partial b_l}$$<br>\n",
    "Where the sums are over all the training examplpes $X_j$ in the current mini-batch. Then pick out another randomly chosen mini-batch and train with those. And so on until we've exhauste the training inputs, which is said to complete an <strong>epoch</strong> of training. And then we star new epoch.<br>\n",
    "\n",
    "The whole epoch process with the MNIST is:<br>\n",
    "![epoch](/notebooks/imgs/epoch.jpg)<br>\n",
    "\n",
    "Do not need to wory about viaulize for above 3-D, no one can do it even for the professional mathmaticians. But can use the algebra like the gradient descent to solve this problem another way and there are lots of different techology for this.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing our network to classify digits\n",
    "Apply what we have till now on the MNIST.<br>\n",
    "Split 60,000-images MNIST training set into 2 parts: 50,000 traning set and 10,000 validation set.<br>\n",
    "\n",
    "The centerpiece is a <i>Network</i> class, use to represent a neural network.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.rand(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x,y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code, the <i>sizes</i> contains the # of neurons for each layer. If we want 2 for 1st layer, 3 for 2nd and 1 for final layer the code will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network([2, 3, 1])\n",
    "net.sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biases and weights in the network are all initialized randomly, with Gaussian distributions with mean 0 and sd 1. This is the start point for the gradient descent. The first layer is default as input layer which do not have biases.<br>\n",
    "Both are storeda as lists of Numpy matrices.<br>\n",
    "Denote $w_{jk}$ is the weight for the connection between the $k^{th}$ in the 2nd layer and $j^{th}$ neuron in the 3rd layer. The vector of activations of the 3rd layer of neurons is:<br>\n",
    "$$a' = \\sigma(wa+b)$$<br>\n",
    "$a$ is the vector of activations of the second layer of neurons.<br>\n",
    "\n",
    "Then, define the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $z$ is a vector, the numpy will do it elementwise.<br>\n",
    "Then add fed forward method which take input and return corresponding output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feedforward(self, a):\n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "        a = sigmoid(np.dot(w, a) + b)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the next part will be the learning part, SGD, stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size]\n",
    "                           for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batches in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>training_data</i> is a list of (x,y) representing the training inputs and corresponding outputs. <i>epochs</i> and <i>mini_batch_size</i> are the number of epochs to train and the size of the mini-batches to use when sampling. <i>eta</i> is the learning rate, $\\eta$. If <i>test_data</i> is supplied, the program will evaluate the network after each epoch of training and print out partial progress.<br>\n",
    "The works as follows, in each epoch, it starts by randomly shuffling the traning data, then partitions into mini-batches. For each mini_batch apply single step of GD and update the weights and biases.<br>\n",
    "The update methos is:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x,y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the work is done in the <strong> backpropagation</strong> algorithm, a fast way of computing the gradient of the cost function.<br>\n",
    "The full code is follow:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the SGD, stochastic gradient dscent o learn from the MNIST training_data for 30 epochs, w/ mini_batch_size 10 and learning rate 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Network([784, 30, 10])\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If change the neuron to 100, the result to will be improved. In this case using more hidden neurons helps get better results.<br>\n",
    "\n",
    "<strong>Hyper-parameters</strong><br>\n",
    "To obtain these accuracies had to make specific choices for the number of epochs, mini-batch size, and the learning rate, $\\eta$. There are different from the weights and biases, which learnt by learning algorithm. For example, if choose $\\eta=0.001$, the result will be bad, but the result is getting slowly better over time. This indicate we can increase the learning rate, e.g. $\\eta = 0.01$. <br>\n",
    "If making a change improves things, try doing more. So even we make poor choice when initial it, we can have the information to improve that.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Support Vector Machine</strong><br>\n",
    "Try SVM on the same examplpe with the python libarary scikit-learn, LIBSVM.<br>\n",
    "sophisticaed algorithm $\\leq$ simple learning algorithm + good training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toward deep learning<br>\n",
    "Architecture for how networks function when try to recognize if the picture have a face, rectangle is for the sub-networks:<br>\n",
    "![arch2](/notebooks/imgs/arch2.jpg)<br>\n",
    "And the sub-networks acan also be decomposed:<br>\n",
    "![arch3](/notebooks/imgs/arch3.jpg)<br>\n",
    "And these can be broken down, further and further through multiple layers. So the end result is a network which breaks down a complicaated question to very simple questions answerable at the level of single pixels. It does this through a series of many layers, w/ early layers answering very simmple and specific question about the input images, and later layers building up a hierarchy of ever more complex and abstract concepts. Networks like this named as <strong>deep neural networks</strong><br>\n",
    "Earlyier, they try to use the SGD and backpropagation to train deep networks, but it's too slowly to be useful.<br>\n",
    "\n",
    "2006, set of techniques developed that enable learning in deep neural nets. These deep learning techiniques are based on SGD and backpropagation, but also introduce new ideas. Now, people can train w/ 5 to 10 hidden layers. And these perform far better than shallow neural networks, network w/ just single layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
