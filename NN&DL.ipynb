{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy;Copyright 2017 Shuang Wu<br>\n",
    "cite from the Neural Networks and Deep Learning book by Michael Nielsen <br>\n",
    "Learning notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NN & DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CH 1\n",
    "## Using NN to recognize handwritten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Human vision involves entire series of visual cortices $V1$, $V2$, $\\cdots$ $V5$, doing complex image processing. We are stupendously, astoundingly good at making sense of what our eyes show us, but done unconscilously.<br>\n",
    "![handW1](imgs/handW1.jpg)\n",
    "Computer recognize is more difficult. The shape of \"9\" which include a loop and a top is hard to transfer to algorithm. When try use this rule, always hopeless.<br>\n",
    "\n",
    "Idea for NN is take large number of handwritten digits, training examples, and then develop a system learn from training examples. NN use examples automatically infer rules to do the recogniz. Increase the # of examples will increase the accuracy.<br>\n",
    "![handW2](imgs/handW2.jpg)\n",
    "NN are used by banks to process cheques and post offices to recognize addresses.          \n",
    "Handwritting reognition is a good prototype example for learning NN in general. And can also apply to speech, natural language processing, etc..<br>\n",
    "\n",
    "2 Important types of artificial neuron:\n",
    "  1. Perceptron\n",
    "  2. Sigmoid neuron<br>\n",
    "  \n",
    "NN Standard learning algorithm: stochastic gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percetron\n",
    "Perceptron takes binary inputs and produces a single binary output:<br>\n",
    "![percep1](imgs/percep1.jpg)\n",
    "The neuron's output, 0 or 1, is determined by the weighted sum $Sigma_j w_jx_j$ is less than or greater than some threshold value, 0 or 1.<br>\n",
    "![percep2](imgs/percep2.jpg)\n",
    "By varying the weights and the threshold, we can get diff. models of decision-making.<br>   \n",
    "Perceptron isn't a complete model of human decision-making, but a complex networks of perceptrons could make quite subtle decisions:<br>\n",
    "![percep3](imgs/percep3.jpg)\n",
    "The 1st Layer making three decisions. 2nd making four decisions by weighing up the results from the 1st layer. 2nd will make more complex and abstract decisions than 1st layer. 3rd layer make more complex decision than 2nd. Many-layer network of percetrons can engage in sophisticated decision making.<br>\n",
    "<br><br>\n",
    "<strong>Math</strong>:    \n",
    "$w\\cdot{x}\\equiv\\Sigma_j w_jx_j$, $w$ and $x$ are vectors for weights and inputs. $b\\equiv\\text{-threshold}$, $b$ is perceptron's bias.<br>\n",
    "\n",
    "$$\\text{output} = \\begin{cases} 0 & \\quad \\text{if}\\quad w\\cdot x+b\\leq 0\\\\\n",
    "                                              1 & \\quad \\text{if}\\quad w\\cdot x+b> 0\\\\ \\end{cases}$$  \n",
    "\n",
    "NAND, not AND:<br>\n",
    "A perceptron with weights -2, -2 and a bias 3:<br>\n",
    "  1. when input 00, (-2) * 0 + (-2) * 0 + 3 = 3, positive 1. not (F&F) = T\n",
    "  2. when input 10 or 01, (-2) * 1 + (-2) * 0 + 3 = 1, positive 1. not (F&T) = T\n",
    "  3. when input 11, (-2) * 1 + (-2) * 0 + 3 = -1, negative 0. not (T&T) = F\n",
    "Can use perceptron to compute simple logical functions. Can conput any logical function. NAND gates are universal for computtion, same as perceptrons.<br>\n",
    "\n",
    "NAND to bitwise sum:<br>\n",
    "![percep4](imgs/percep4.jpg)<br>\n",
    "![percep5](imgs/percep5.jpg)<br>\n",
    "![percep6](imgs/percep6.jpg)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid neurons (Sigmoid also named as logistic)\n",
    "\n",
    "For the learning algorithms, we want small change to the weights or boas will also have small change in output. Than we can modify to let the network behave more in manner we want. BUT not when include single perceptron in network. Small change may cause result flip and cause output complete change.<br>\n",
    "\n",
    "Sigmoid neuron, artificial neuron to solve the problem. Similar to perceptrons but small change gain small change, the crucial fact allow network of sigmoid neuron to learn.<br>\n",
    "![sigmoid1](imgs/sigmoid1.jpg)<br>\n",
    "\n",
    "Like perceptron, sigmoid neuron has inputs. The values can take not only 0, 1 but also between 0 and 1, like 0.6. Have the weights and bias like before. Output is not 0 or 1, it's $\\sigma (w{\\cdot}x+b)$ where $\\sigma\\equiv\\frac{1}{1+e^{-z}}$<br>\n",
    "\n",
    "Sigmoid neuron with input, weights and bias is:<br>\n",
    "$$\\frac{1}{1+exp(-\\Sigma_j w_jx_j-b)}$$\n",
    "\n",
    "Sigmoid function:<br>\n",
    "![sigmoid1](imgs/sigmoid2.jpg)<br>\n",
    "When $z$ goes to $+\\infty$, value goes to 1, vice versa. Smoothed out version of a step func.<br>\n",
    "And we can calculate the how the change of weights and change of bias will affect the change of output:<br>\n",
    "\n",
    "The output of sig. neuron with inputs, weights and bias is:<br>    \n",
    "$$\\Delta\\text{output}\\approx\\sum_{j}\\frac{\\partial\\text{output}}{\\partial{w_j}}\\Delta{w_j}+\\frac{\\partial\\text{output}}{\\partial{b}}\\Delta{b}$$ \n",
    "\n",
    "The equation says: $\\Delta\\text{output}$ is a <i>linear function</i> of the changes $\\Delta w_j$ and $\\Delta b$ in the weights and bias. Much easier to figure out how changing the weights and biases will change the output.<br>\n",
    "\n",
    "$\\sigma$ use widely because the pretty derivative properties, commonly-used in work on neural nets.<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The architeture of neural networks\n",
    "\n",
    "![arch1](imgs/arch1.jpg)<br>\n",
    "From left to right: input layer with input neurons. Hidden, not input or output, layer with hidden neurons. Output layer with output neuron. Hidden can have single or multiple layer.<br>\n",
    "\n",
    "Sometimes multiple layer networks called multilayer perceptrons, <strong>MLP</strong>s, even made by sigmoid.<br>\n",
    "\n",
    "<i>Feedforward neural networks</i>, the output from one layer is used as input to the next layer. No loops in the network for fed forward.<br>\n",
    "\n",
    "<i>Recurrent neural networks</i>, have the feedback loops, use the output for the input. Learning algorithms less powerful, but much closer in spirit to how our brains work than fed f. Can solve important problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simplle network to classify handwritten\n",
    "\n",
    "Split into 2 problem:<br>\n",
    "1. Breaking an image with digits to separate images, each containing single digit.<br>\n",
    "![handw3](imgs/handw3.jpg)<br>\n",
    "2. Classify each individual digit, focus on this problem.<br>\n",
    "\n",
    "To solve the recognizing individual handwritten digits problem, using 3-layer neural network:<br>\n",
    "![handw4](imgs/handw4.jpg)<br>\n",
    "The input layer contains $784=28*28$ neurons because the input image is 28 by 28 pixel. The pixels are greyscale with 0 for white and 1 for black, after normalize by divided 255. The second layer is hidden layer with value n neurous. The output layer have 10 neurons, with the value from 0 to 1. The sum of all these 10 values will be 1 and the largest one from the 10 neurons decided the image's digit, e.g. first neuron is largest means input number is 0.<br>\n",
    "\n",
    "4 output neurons with binary 0 or 1 is already enough to have all 10 situation, but the 10 output neurons have the best prediction. If use 4 outputs, the first neuron would be trying to decide the most significant bit of the digit was, which is not realistic. Just heuristic, can try whatever you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with gradient descent\n",
    "\n",
    "First, need data set to learn from, training data set (MNIST).<br>\n",
    "The data set has 2 parts. 1st, 60,000 images with labels as training data. 2nd, 10,000 images with labels as test data. All $28*28$ grescale images. Test data written by different people compare to the training data.<br>\n",
    "\n",
    "$x$ denote training input, 784-dim vector. Output is $y=y(x)$, 10-dim col. vector. We want find weights and biases for $\\hat{y}\\approx y=y(x)$, the $\\hat{y}$ is the estimator.<br>\n",
    "\n",
    "To quantify how well we're achieving, define <i>cost function</i>, or loss, objective function: <br>\n",
    "$$C(w, b)\\equiv\\frac{1}{2n}\\sum_{x} \\|y(x)-a\\|^2$$\n",
    "$w$ is a vector contain all weights, $b$ is a vector contain all biases, $n$ is the # of total trining inputs. $a$ is the output denpends on $x$, $w$, and $b$. $\\|v\\|$ is the length function for vector $v$. C is the <i>quadratic</i> cost function.<br>\n",
    "Cost function is non-negative and becomes small, approx to 0, when $y(x)$ approx equal to output, $a$, for all training inputs, $x$.<br>\n",
    "\n",
    "The goal is to minimize the cost function $C(w, b)$, we use the algorithm known as <i>gradient descent</i>. <br>\n",
    "We do not dirrectly look the number of accuracy becuase we need to find smooth relation between the outputs and the weights and biases. Thats why we look at the quadratic cost function, easy to make small changes in weights and biases to inprove the cost.<br>\n",
    "<br>\n",
    "\n",
    "<strong>Gradient descent</strong>\n",
    "$$\\min_{C(v)}$$\n",
    "for $v = v_1, v_2,\\cdots$. Suppose when consider of just 2 variables $v_1$ and $v_2$:\n",
    "![gs1](imgs/gs1.jpg)<br>\n",
    "It's easy to found the global minimum from the above plot.<br>\n",
    "One way to solve is to use calculus to find the minimum analytically. Comput derivatives and then try using them to find places where C is an extremum. But work when C is a function of just one or few variables. <strong>Not for Neural networks.</strong><br>\n",
    "\n",
    "Move the ball a small amount $\\Delta v_1$ in the $v_1$ direction, and a small amount $\\Delta v_2$ in the $v_2$ direction. $C$ changes as follows:<br>\n",
    "$$\\Delta C \\approx\\frac{\\partial C}{\\partial v_1}\\Delta v_1 + \\frac{\\partial C}{\\partial v_2}\\Delta v_2$$<br>\n",
    "Going to choose  $\\Delta v_1$ and $\\Delta v_2$ so make $$\\Delta C$$ negative. Define $\\Delta v$ to be the vector of changes in $v$, define the gradient of $C$:<br>\n",
    "$$\\nabla{C}\\equiv(\\frac{\\partial C}{\\partial v_1},\\frac{\\partial C}{\\partial v_2})^T$$\n",
    "$$\\Delta C \\approx \\nabla C\\cdot\\Delta v$$<br>\n",
    "\n",
    "Choose of $\\Delta v$ to make the $\\Delta C$ negative, suppose we choose:<br>\n",
    "$$\\Delta v = -\\eta\\nabla C$$<br>\n",
    "where $\\eta$ is a small, positive parameter, known as <i>learning rate</i>, then the equation change to:<br>\n",
    "$$\\Delta C \\approx -\\eta\\nabla C\\cdot\\nabla C = -\\eta\\|\\nabla C\\|^2$$<br>\n",
    "because $\\|\\nabla C\\|^2\\geq 0 $, so $\\Delta C \\leq 0$, C will always decreaase.<br>\n",
    "\n",
    "This is what we look for and will use this to calculate $\\Delta v$ and move the position $v$ by that amount:<br>\n",
    "$$v \\rightarrow v' = v - \\eta\\nabla C$$<br>\n",
    "We use this rule to keep updating the value to decreasing $C$ until reach the \"global minimum\", can visualize like this:<br>\n",
    "![gs2](imgs/gs2.jpg)<br>\n",
    "\n",
    "To make the dradient descent work, need to choose the learning rate $\\eta$ to be small enough. If not, may end up with $\\Delta C > 0$, but if learning rate to small, it will take a long time to find the minimum. Same thing when more than 2 variables.<br>\n",
    "\n",
    "Gradient descent with the neuron networks. The updating rule with weights and biases are:<br>\n",
    "$$w_k \\rightarrow w'_k = w'_k-\\eta\\frac{\\partial C}{\\partial w_k}$$<br>\n",
    "$$b_l \\rightarrow b'_l = b'_l-\\eta\\frac{\\partial C}{\\partial b_l}$$<br>\n",
    "By updating these, reach the minimum of the cost function. But when the # if training inputs is very large this will take long time, the <i>stochastic gradient descent</i> can used to speed up. The idea is to estimate the gradient $\\nabla C$ by computing $\\nabla C_x$ for a small sample of randomly chosen training inputs.<br>\n",
    "\n",
    "<strong>Stochastic gradient descent</strong><br>\n",
    "selec m of randomly chosen training inputs. Provided the sample size m is large enough, we want the gradient value roughly equl:<br>\n",
    "$$\\frac{\\sum_{j=1}^{m}\\nabla C_{x_{j}}}{m}\\approx \\frac{\\sum_{x}\\nabla C_{x}}{n} = \\nabla C$$<br>\n",
    "Stochastic gradient descent works by picking out a randomly chose nmini-batch of training inputs, and training with those:<br>\n",
    "$$w_k \\rightarrow w'_k = w_k-\\frac{\\eta}{m}\\sum_{j}\\frac{\\partial C_{x_j}}{\\partial w_k}$$<br>\n",
    "$$b_l \\rightarrow b'_l = b_l-\\frac{\\eta}{m}\\sum_{j}\\frac{\\partial C_{x_j}}{\\partial b_l}$$<br>\n",
    "Where the sums are over all the training examplpes $X_j$ in the current mini-batch. Then pick out another randomly chosen mini-batch and train with those. And so on until we've exhauste the training inputs, which is said to complete an <strong>epoch</strong> of training. And then we star new epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
