{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy;Copyright 2017 Shuang Wu<br>\n",
    "cite from the Neural Networks and Deep Learning book by Michael Nielsen <br>\n",
    "Learning notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NN & DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CH 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NN to recognize handwritten\n",
    "Human vision involves entire series of visual cortices $V1$, $V2$, $\\cdots$ $V5$, doing complex image processing. We are stupendously, astoundingly good at making sense of what our eyes show us, but done unconscilously.<br>\n",
    "![handW1](imgs/handW1.jpg)\n",
    "Computer recognize is more difficult. The shape of \"9\" which include a loop and a top is hard to transfer to algorithm. When try use this rule, always hopeless.<br>\n",
    "\n",
    "Idea for NN is take large number of handwritten digits, training examples, and then develop a system learn from training examples. NN use examples automatically infer rules to do the recogniz. Increase the # of examples will increase the accuracy.<br>\n",
    "![handW2](imgs/handW2.jpg)\n",
    "NN are used by banks to process cheques and post offices to recognize addresses.          \n",
    "Handwritting reognition is a good prototype example for learning NN in general. And can also apply to speech, natural language processing, etc..<br>\n",
    "\n",
    "2 Important types of artificial neuron:\n",
    "  1. Perceptron\n",
    "  2. Sigmoid neuron<br>\n",
    "  \n",
    "NN Standard learning algorithm: stochastic gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percetron\n",
    "Perceptron takes binary inputs and produces a single binary output:<br>\n",
    "![percep1](imgs/percep1.jpg)\n",
    "The neuron's output, 0 or 1, is determined by the weighted sum $Sigma_j w_jx_j$ is less than or greater than some threshold value, 0 or 1.<br>\n",
    "![percep2](imgs/percep2.jpg)\n",
    "By varying the weights and the threshold, we can get diff. models of decision-making.<br>   \n",
    "Perceptron isn't a complete model of human decision-making, but a complex networks of perceptrons could make quite subtle decisions:<br>\n",
    "![percep3](imgs/percep3.jpg)\n",
    "The 1st Layer making three decisions. 2nd making four decisions by weighing up the results from the 1st layer. 2nd will make more complex and abstract decisions than 1st layer. 3rd layer make more complex decision than 2nd. Many-layer network of percetrons can engage in sophisticated decision making.<br>\n",
    "<br><br>\n",
    "<strong>Math</strong>:    \n",
    "$w\\cdot{x}\\equiv\\Sigma_j w_jx_j$, $w$ and $x$ are vectors for weights and inputs. $b\\equiv\\text{-threshold}$, $b$ is perceptron's bias.<br>\n",
    "\n",
    "$$\\text{output} = \\begin{cases} 0 & \\quad \\text{if}\\quad w\\cdot x+b\\leq 0\\\\\n",
    "                                              1 & \\quad \\text{if}\\quad w\\cdot x+b> 0\\\\ \\end{cases}$$  \n",
    "\n",
    "NAND, not AND:<br>\n",
    "A perceptron with weights -2, -2 and a bias 3:<br>\n",
    "  1. when input 00, (-2) * 0 + (-2) * 0 + 3 = 3, positive 1. not (F&F) = T\n",
    "  2. when input 10 or 01, (-2) * 1 + (-2) * 0 + 3 = 1, positive 1. not (F&T) = T\n",
    "  3. when input 11, (-2) * 1 + (-2) * 0 + 3 = -1, negative 0. not (T&T) = F\n",
    "Can use perceptron to compute simple logical functions. Can conput any logical function. NAND gates are universal for computtion, same as perceptrons.<br>\n",
    "\n",
    "NAND to bitwise sum:<br>\n",
    "![percep4](imgs/percep4.jpg)<br>\n",
    "![percep5](imgs/percep5.jpg)<br>\n",
    "![percep6](imgs/percep6.jpg)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid neurons (Sigmoid also named as logistic)\n",
    "\n",
    "For the learning algorithms, we want small change to the weights or boas will also have small change in output. Than we can modify to let the network behave more in manner we want. BUT not when include single perceptron in network. Small change may cause result flip and cause output complete change.<br>\n",
    "\n",
    "Sigmoid neuron, artificial neuron to solve the problem. Similar to perceptrons but small change gain small change, the crucial fact allow network of sigmoid N to learn.<br>\n",
    "\n",
    "Like perceptron, sigmoid N has inputs. The values can take not only 0, 1 but also between 0 and 1, like 0.6. Have the weights and bias like before. Output is not 0 or 1, it's$\\sigma (w{\\cdot}x+b)$ where $\\sigma\\equiv\\frac{1}{1+e^{-z}}$<br>\n",
    "\n",
    "Sigmoid neuron with input, weights and bias is:<br>\n",
    "$\\frac{1}{1+exp(-\\Sigma_jw_jx_j-b)}$\n",
    "\n",
    "Sigmoid function:<br>\n",
    "When $z$ goes to $+\\inf$, value goes to 1, vice versa. Smoothed out version of a step func.<br>\n",
    "And we can calculate the how the change of weights and change of bias will affect the change of output:<br>\n",
    "![sigmoid1](imgs/sigmoid1.jpg)<br>\n",
    "\n",
    "The output of sig. N with inputs, weights and bias is:<br>    \n",
    "$$\\Delta\\text{output}\\approx\\Sigma_{j}\\frac{\\partial\\text{output}}{\\partial{w_j}}\\Delta{w_j}+\\frac{\\partial\\text{output}}{\\partial{b}}\\Delta{b}$$ \n",
    "\n",
    "The equation says: Change of putput is a linear function of the changes of \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
