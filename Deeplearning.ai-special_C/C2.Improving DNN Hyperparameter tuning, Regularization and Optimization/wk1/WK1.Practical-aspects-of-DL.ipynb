{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy;Copyright for [Shuang Wu] [2017]<br>\n",
    "Cite from the [coursera] named [Neural network and Machine Learning] from [deeplearning.ai]<br>\n",
    "Learning notes<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up your ML app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Dev / Test sets\n",
    "\n",
    "* Iterative process to find:\n",
    "    * #of layers\n",
    "    * #of hidden units\n",
    "    * learning rates\n",
    "    * activation\n",
    "    * etc.\n",
    "    \n",
    "* Train / Dev / Test sets\n",
    "    * Training set\n",
    "        * pre: 70%/60%\n",
    "        * big data: 98%/99.5%\n",
    "    * Hold-out cros validation set (Development set)('dev')\n",
    "        * pre: 20%\n",
    "        * big data: 1%\n",
    "    * Test set\n",
    "        * pre: 30%/20%\n",
    "        * big data: 1%\n",
    "\n",
    "* Mismatched train/test distr.\n",
    "    * training set:\n",
    "        * cat picturec from webpages\n",
    "    * Dev/test sets:\n",
    "        * cat pictures from users using your app\n",
    "    * then ditr. from this two set maybe different\n",
    "    * Make sure dev and test come from same distribution\n",
    "    * Not having a test set might be okay. (Only dev set.)\n",
    "    \n",
    "## Bias and Variance\n",
    "\n",
    "* Hight bias\n",
    "    * underfitting\n",
    "        * Train set error: 15%\n",
    "        * Dec set error:16%\n",
    "* Just right\n",
    "    * Train set error: 0.5%\n",
    "    * Dec set error:1%\n",
    "* High variance\n",
    "    * overfitting\n",
    "        * Train set error: 1%\n",
    "        * Dec set error:11%\n",
    "* Both high bais & variance:\n",
    "    * Train set error: 15%\n",
    "    * Dec set error:30%\n",
    "* For human, optimal (Bayes) error: 15%\n",
    "    * Blury image\n",
    "    \n",
    "## Basic \"recipe\" for ML\n",
    "\n",
    "* If High bias (training data performance)\n",
    "    * Try Bigger network\n",
    "    * training longer\n",
    "    * NN architecture search\n",
    "* If High variance ( dev set performance)\n",
    "    * try more data\n",
    "    * try regularization\n",
    "    * NN arch. search\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Regularizing NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "* logistic regression\n",
    "    * L2 regularization\n",
    "        * $+\\frac{\\lambda}{2m}\\|w\\|^2_2$\n",
    "        * $\\|w\\|^2_2 = \\sum_{j=1}^{n_x}w_j^2=w^Tw$\n",
    "    * L1 regularization\n",
    "        * $\\frac{\\lambda}{2m}\\sum_{j=1}^{n_x}|w_j|=\\frac{\\lambda}{2m}\\|w\\|_1$\n",
    "    * Lamda\n",
    "        * regularization parameter\n",
    "\n",
    "* NN\n",
    "    * $+\\frac{\\lambda}{2m}\\sum^L_{l=1}\\|w^{[l]}\\|^2_F$\n",
    "    * Forbenius norm for matrix: \n",
    "        * $\\|w^{[l]}\\|^2_F=\\sum_{i=1}^{n^{[l-1]}}\\sum_{j=1}^{n^{[l]}}(w_{ij}^{[l]})^2$\n",
    "    * GD:\n",
    "        * $dw^{[l]} = \\text{(from backprop)} + \\frac{\\lambda}{m}w^{[l]}$\n",
    "        * $w^{[l]} := w^{[l]} - \\alpha dw^{[l]}$\n",
    "        * $\\frac{\\partial J}{\\partial w^{[l]}}= dw^{[l]}$\n",
    "        \n",
    "## Why regularizationg reduces overfitting\n",
    "\n",
    "* when $lambda$ is really large, may set those w close to zero, then zeroing out some hidden neurons which can from high variacne to high bias\n",
    "    * smaller network\n",
    "* when lambda large, the weights will be relative small, smake as z, then every layer will approx to linear and the whole NN will be near liner \n",
    "\n",
    "## Dropout regularization\n",
    "\n",
    "* Dropout Regularization\n",
    "* Implementing dropout (\"Inverted dropout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
