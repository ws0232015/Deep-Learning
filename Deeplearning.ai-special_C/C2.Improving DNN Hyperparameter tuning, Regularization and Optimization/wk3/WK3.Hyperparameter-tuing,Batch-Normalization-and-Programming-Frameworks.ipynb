{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy;Copyright for [Shuang Wu] [2017]<br>\n",
    "Cite from the [coursera] named [Neural network and Machine Learning] from [deeplearning.ai]<br>\n",
    "Learning notes<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning process\n",
    "\n",
    "* Hyperparameters\n",
    "    * $\\alpha$, 1st important\n",
    "    * $\\beta$, momentum, 0.9, 2nd important\n",
    "    * $\\beta_1$, always use default\n",
    "    * $\\beta_2$, always use default\n",
    "    * $\\epsilon$, always use default\n",
    "    * #of layers, 3rd important\n",
    "    * #of hidden units, 2nd important\n",
    "    * learning rate decay, 3rd important\n",
    "    * mini-batch size, 2nd important\n",
    "    \n",
    "* Try random values: Don't use a grid\n",
    "    * ![img1](imgs/img1.jpg)\n",
    "    \n",
    "* Coarse to fine\n",
    "    * ![img2](imgs/img2.jpg)\n",
    "    * sample and then zoom in and sample more "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an appropriate scale to pick hyperparameters\n",
    "\n",
    "* Picking hyperparameters at random\n",
    "    * ![img3](imgs/img3.jpg)\n",
    "    * #of layers L: maybe 2-4\n",
    "\n",
    "* Appropriate scale for hyperparameters\n",
    "    * if $\\alpha=0.0001$ to $1$\n",
    "        * search the value on a log scale\n",
    "        * not sample uniform random \n",
    "        * ```python\n",
    "        r=-4*np.randn.rand() #[-4,0]\n",
    "        alpha=10**r #10^-4...10^0\n",
    "        ```\n",
    "        \n",
    "* Hyperparameters for exponentially weighted averages\n",
    "    * $\\beta=0.9,\\cdots,0.999$\n",
    "    * $1-\\beta=0.1,\\cdots,0.001$\n",
    "    * such as before sample only (0.1, 0.01, 0.001)\n",
    "    * $\\beta$ from 0.9000 to 0.9005 will not make that huge change\n",
    "    * but $\\beta$ from 0.9990 to 0.9995 will make huge change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning in practive: pandas vs. Caviar\n",
    "\n",
    "* Re-test hyperparameters occasionally\n",
    "    * Idea to code to experiment to idea\n",
    "        * NLP, vision, speech, Ads, logistics,...\n",
    "        * Intuitions do get stale. Re-evaluate occasionally\n",
    "\n",
    "* Babysitting one model\n",
    "    * ![img4](imgs/img4.jpg)\n",
    "    * big data w/o enough cpu or gpu\n",
    "        * panda\n",
    "    \n",
    "* Training many models in parallel\n",
    "    * ![img5](imgs/img5.jpg)\n",
    "        * caviar (fish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizaing activations in a network\n",
    "    \n",
    "* Normalizing inputs to speed up learning\n",
    "    * ![img6](imgs/img6.jpg)\n",
    "    * this work for w,b not for deep NN\n",
    "    \n",
    "* BN \n",
    "    * normalize the Z val.\n",
    "    \n",
    "* Implementing Batch Norm\n",
    "    * Give some intermedient val. in NN $Z^{(1)}$, ..., $Z^{(m)}$\n",
    "        * $\\mu = \\frac{1}{m}\\sum_i Z^{(i)}$\n",
    "        * $\\sigma^i = \\frac{1}{m}\\sum_i(Z_i-\\mu)^2$\n",
    "        * $$Z^{(i)}_{norm}=\\frac{$Z^{(1)}$-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}$$\n",
    "        * $$\\tilde{Z^{(i)}}=\\gamma Z^{(i)}_{norm}+\\beta$$\n",
    "            * $\\gamma$ and $\\beta$ learnable parameters of model\n",
    "        * If $\\gamma = \\sqrt{\\sigma^2+\\epsilon}$ and $\\beta=\\mu$\n",
    "        * Then $\\tilde{Z^{(i)}}= Z^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Batch Norm into a NN\n",
    "\n",
    "* Adding Batch Norm to a network\n",
    "    * ![img7](imgs/img7.jpg)\n",
    "\n",
    "* Working w/ mini-batches\n",
    "    * ![img8](imgs/img8.jpg)\n",
    "\n",
    "* Implementing gradient descent\n",
    "    * ```python\n",
    "    for t =1...num minibathes\n",
    "        compute forwad prop on X{t}\n",
    "             in each hidden layer use BN to repa z[l] w/ tilde(z)[l]\n",
    "         use backprop to compute dw, db, dbeta, dgamma\n",
    "         update parameters\n",
    "    ```\n",
    "    * ![img9](imgs/img9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does Batch Norm work\n",
    "\n",
    "* Learning on shifting input distribution\n",
    "    * ![img10](imgs/img10.jpg)\n",
    "\n",
    "* Why this is a problem with NN\n",
    "    * \n",
    "    \n",
    "* Batch Norm as regularization\n",
    "    * Each mini-batch is caled by the mean/variance computed on just that mini-batch\n",
    "    * This adds some noise to the values z within that minibatch. So similar to dropout, it adds some noise to each hidden layer's activations\n",
    "    * this has a slight regularization effect\n",
    "        * minibatch: 64 to 514"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Norm at test time\n",
    "\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
