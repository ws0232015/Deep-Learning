{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy;Copyright for [Shuang Wu] [2017]<br>\n",
    "Cite from the [coursera] named [Neural network and Machine Learning] from [deeplearning.ai]<br>\n",
    "Learning notes<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning process\n",
    "\n",
    "* Hyperparameters\n",
    "    * $\\alpha$, 1st important\n",
    "    * $\\beta$, momentum, 0.9, 2nd important\n",
    "    * $\\beta_1$, always use default\n",
    "    * $\\beta_2$, always use default\n",
    "    * $\\epsilon$, always use default\n",
    "    * #of layers, 3rd important\n",
    "    * #of hidden units, 2nd important\n",
    "    * learning rate decay, 3rd important\n",
    "    * mini-batch size, 2nd important\n",
    "    \n",
    "* Try random values: Don't use a grid\n",
    "    * ![img1](imgs/img1.jpg)\n",
    "    \n",
    "* Coarse to fine\n",
    "    * ![img2](imgs/img2.jpg)\n",
    "    * sample and then zoom in and sample more "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an appropriate scale to pick hyperparameters\n",
    "\n",
    "* Picking hyperparameters at random\n",
    "    * ![img3](imgs/img3.jpg)\n",
    "    * #of layers L: maybe 2-4\n",
    "\n",
    "* Appropriate scale for hyperparameters\n",
    "    * if $\\alpha=0.0001$ to $1$\n",
    "        * search the value on a log scale\n",
    "        * not sample uniform random \n",
    "        * ```python\n",
    "        r=-4*np.randn.rand() #[-4,0]\n",
    "        alpha=10**r #10^-4...10^0\n",
    "        ```\n",
    "        \n",
    "* Hyperparameters for exponentially weighted averages\n",
    "    * $\\beta=0.9,\\cdots,0.999$\n",
    "    * $1-\\beta=0.1,\\cdots,0.001$\n",
    "    * such as before sample only (0.1, 0.01, 0.001)\n",
    "    * $\\beta$ from 0.9000 to 0.9005 will not make that huge change\n",
    "    * but $\\beta$ from 0.9990 to 0.9995 will make huge change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning in practive: pandas vs. Caviar\n",
    "\n",
    "* Re-test hyperparameters occasionally\n",
    "    * Idea to code to experiment to idea\n",
    "        * NLP, vision, speech, Ads, logistics,...\n",
    "        * Intuitions do get stale. Re-evaluate occasionally\n",
    "\n",
    "* Babysitting one model\n",
    "    * ![img4](imgs/img4.jpg)\n",
    "    * big data w/o enough cpu or gpu\n",
    "        * panda\n",
    "    \n",
    "* Training many models in parallel\n",
    "    * ![img5](imgs/img5.jpg)\n",
    "        * caviar (fish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizaing activations in a network\n",
    "    \n",
    "* Normalizing inputs to speed up learning\n",
    "    * ![img6](imgs/img6.jpg)\n",
    "    * this work for w,b not for deep NN\n",
    "    \n",
    "* BN \n",
    "    * normalize the Z val.\n",
    "    \n",
    "* Implementing Batch Norm\n",
    "    * Give some intermedient val. in NN $Z^{(1)}$, ..., $Z^{(m)}$\n",
    "        * $\\mu = \\frac{1}{m}\\sum_i Z^{(i)}$\n",
    "        * $\\sigma^i = \\frac{1}{m}\\sum_i(Z_i-\\mu)^2$\n",
    "        * $$Z^{(i)}_{norm}=\\frac{Z^{(1)}-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}$$\n",
    "        * $$\\tilde{Z^{(i)}}=\\gamma Z^{(i)}_{norm}+\\beta$$\n",
    "            * $\\gamma$ and $\\beta$ learnable parameters of model\n",
    "        * If $\\gamma = \\sqrt{\\sigma^2+\\epsilon}$ and $\\beta=\\mu$\n",
    "        * Then $\\tilde{Z^{(i)}}= Z^{(i)}$\n",
    "    * use $\\tilde{Z^{(i)}}$ instead of $Z^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Batch Norm into a NN\n",
    "\n",
    "* Adding Batch Norm to a network\n",
    "    * ![img7](imgs/img7.jpg)\n",
    "\n",
    "* Working w/ mini-batches\n",
    "    * ![img8](imgs/img8.jpg)\n",
    "\n",
    "* Implementing gradient descent\n",
    "    * ```python\n",
    "    for t =1...num minibathes\n",
    "        compute forwad prop on X{t}\n",
    "             in each hidden layer use BN to repa z[l] w/ tilde(z)[l]\n",
    "         use backprop to compute dw, db, dbeta, dgamma\n",
    "         update parameters\n",
    "    ```\n",
    "    * for $t=1$ ... num MiniBatches\n",
    "        * compute forward prop on $X^{\\{t\\}}$\n",
    "            * In each hidden layer, use BN to replace $z^{[l]}$ with $\\tilde{z}^{[l]}$\n",
    "        * Use backprop to compute the $dw^{[l]}$, $db^{[l]}$, $d\\beta^{[l]}$, $d\\gamma^{[l]}$\n",
    "        * Update parameters:\n",
    "            * $w^{[l]} = w^{[l]}-\\alpha dw^{[l]}$\n",
    "            * $\\beta^{[l]} = \\beta^{[l]}-\\alpha d\\beta^{[l]}$\n",
    "            * $\\gamma^{[l]} = \\gamma^{[l]}-\\alpha d\\gamma^{[l]}$\n",
    "    * works w/ momentum, RMSprop, Adam, Gradient descent\n",
    "    * the constant added, the $b$, will be cancel when do mean subtraction, and replace by the parameter $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does Batch Norm work\n",
    "\n",
    "* Learning on shifting input distribution\n",
    "    * ![img10](imgs/img10.jpg)\n",
    "\n",
    "* Why this is a problem with NN\n",
    "    * ![img11](imgs/img11.jpg)\n",
    "    \n",
    "* Batch Norm as regularization\n",
    "    * Each *mini-batch* is caled by the mean/variance computed on just that mini-batch\n",
    "    * This adds some noise to the values $z^{[l]}$, to $\\tilde{z}^{[l]}$, within that minibatch. So similar to dropout, it adds some noise to each hidden layer's activations\n",
    "    * this has a slight regularization effect\n",
    "        * minibatch size from 64 to 512\n",
    "            * bigger mini-bachsize reduce the regularization effect\n",
    "            * but mini-batch not intend to regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Norm at test time\n",
    "\n",
    "* Batch norm at test time\n",
    "    * $$\\mu=\\frac{1}{m}\\sum_i z^{(i)}$$\n",
    "        * $m$, number of example for minibatch\n",
    "        * only avaliable when training\n",
    "    * $$\\sigma^2=\\frac{1}{m}\\sum_i (z^{(i)}-\\mu)^2$$\n",
    "        * only avaliable when training\n",
    "    * $$z_{norm}^{(i)}=\\frac{z^{(i)}-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}$$\n",
    "    * $$\\tilde{z}^{(i)} = \\gamma z_{norm}^{(i)}+\\beta$$\n",
    "    * $\\mu$, $\\sigma^2$: estimate using exponentialy weighted average across diff. mini-baches\n",
    "        * $X^{\\{1\\}}$, $X^{\\{2\\}}$, $X^{\\{3\\}}$\n",
    "        * $\\mu^{\\{1\\}[l]}$, $\\mu^{\\{2\\}[l]}$, $\\mu^{\\{3\\}[l]}$\n",
    "        * $\\theta_1$, $\\theta_2$, $\\theta_3$\n",
    "        * $\\sigma^{2\\{1\\}[l]}$, $\\sigma^{2\\{2\\}[l]}$, $\\sigma^{2\\{3\\}[l]}$\n",
    "    * In test time\n",
    "        * $$z_{norm}=\\frac{z-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}$$\n",
    "        * $$\\tilde{z}=\\gamma z_{norm}+\\beta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "\n",
    "* Recognizing cats, dogs, and baby chicks\n",
    "    * cats, 1\n",
    "    * dogs, 2\n",
    "    * baby chicks, 3\n",
    "    * other, 0\n",
    "    * ![img12](imgs/img12.jpg)\n",
    "    * $C = #classes =4$ \n",
    "    * ![img13](imgs/img13.jpg)\n",
    "    \n",
    "* Softmax Layer\n",
    "    * ![img14](imgs/img14.jpg)\n",
    "    * \n",
    "    * $$z^{[L]} = w^{[L]}a^{[L-1]}+b^{[L]}$$\n",
    "    * Activation function:\n",
    "        * (4,1) temp var. $$t = e^{z^{[L]}}$$\n",
    "        * $$a^{[L]} = \\frac{e^{z^{[L]}}}{\\sum_{j=1}^4}t_i$$\n",
    "        * $$a^{[L]}_i = \\frac{t_i}{\\sum_{j=1}^4}t_i$$\n",
    "    * e.g.\n",
    "        * $z^{[L]}=\\begin{bmatrix} 5\\\\ 2\\\\ -1\\\\ 3\\end{bmatrix}$,  $t=\\begin{bmatrix} e^5\\\\ e^2\\\\ e^-1\\\\ e^3\\end{bmatrix}=\\begin{bmatrix} 148.4\\\\ 7.4\\\\ 0.4\\\\ 20.1\\end{bmatrix}$\n",
    "        * $$\\sum^4_{j=1}t_j=176.3$$\n",
    "        * $$a^{[L]} = \\frac{t}{176.3}$$\n",
    "        * layer L: $$\\hat{y}=\\begin{bmatrix} \\frac{e^5}{176.3}\\\\ \\frac{e^2}{176.3}\\\\ \\frac{e^-1}{176.3}\\\\ \\frac{e^3}{176.3}\\end{bmatrix}=\\begin{bmatrix} 0.842\\\\ 0.042\\\\ 0.002\\\\ 0.114\\end{bmatrix}$$\n",
    "    * \n",
    "    * $$a^{[L]} = g^{[L]}(z^{[L]})$$\n",
    "        * (4,1) for $a$ and $z$\n",
    "        \n",
    "* softmax examples\n",
    "    * C=3, linear bondary\n",
    "        * ![img15](imgs/img15.jpg)\n",
    "        * ![img16](imgs/img16.jpg)\n",
    "        * ![img17](imgs/img17.jpg)\n",
    "    * C= 4, 5, 6\n",
    "        * ![img18](imgs/img18.jpg)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a softmax classifier\n",
    "\n",
    "* Understanding softmax\n",
    "    * \"hard max\"\n",
    "    * Softmax regression generalizes logistic regression to $C$ classes\n",
    "        * If $C=2$, softmax reduces to logistic regression\n",
    "\n",
    "* Loss function\n",
    "    * $y=\\begin{bmatrix} 0\\\\1\\\\0\\\\0\\end{bmatrix}$, cat\n",
    "    * $a^{[L]}=\\hat{y}=\\begin{bmatrix} 0.3\\\\0.2\\\\0.1\\\\0.4\\end{bmatrix}$, $C=4$\n",
    "    * small the loss: $$l(\\hat(y),y)=-\\sum^4_{j=1}y_jlog\\hat{y}_j$$\n",
    "        * $-y_2log\\hat{y}_2 = -log\\hat{y}_2$\n",
    "        * make $\\hat{y}_2$ big\n",
    "    * cost: $$J(w,b,...)=\\frac{1}{m}\\sum^m_{i=1}l(\\hat{y}^{i},y^{i})$$\n",
    "    \n",
    "* Gradient descent w/ softmax\n",
    "    * ![img19](imgs/img19.jpg)\n",
    "    * \n",
    "    * Backprop: $dz^{[L]}=\\hat{y}-y$, (4,1)\n",
    "        * $\\frac{\\partial J}{\\partial z^{[L]}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to programming frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL frameworks\n",
    "\n",
    "* DL frameworks\n",
    "    * caffe/caffe2\n",
    "    * CNTK\n",
    "    * DL4J\n",
    "    * Keras\n",
    "    * Lasagne\n",
    "    * mxnet\n",
    "    * PaddlePaddle\n",
    "    * TensorFlow\n",
    "    * Theano\n",
    "    * Torch\n",
    "    \n",
    "* Choosing DL frameworks\n",
    "    * Ease of programming (development and deployment)\n",
    "    * Running speed\n",
    "    * Truly open (open source w/ good governance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "\n",
    "* Motivating problem\n",
    "    * Minimize some cost function $J(w)=w^2-10w+25$\n",
    "        * $(w-5)^2$, $w=5$\n",
    "        * $J(w,b)$ to find the $w$, $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#coefficients = np.array([[1.], [-10.], [25.]])\n",
    "coefficients = np.array([[1.], [-20.], [100.]])\n",
    "\n",
    "w = tf.Variable(0, dtype=tf.float32)\n",
    "x = tf.placeholder(tf.float32, [3,1])\n",
    "#cost = tf.add(tf.add(w**2, tf.multiply(-10.,w)),25)\n",
    "#cost = w**2 - 10*w +25\n",
    "cost = x[0][0]*w**2 + x[1][0]*w + x[2][0] #(w-5)**2\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "session.run(train, feed_dict={x:coefficients})\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.99998\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "        session.run(train, feed_dict={x:coefficients})\n",
    "print(session.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ![img20](imgs/img20.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
