{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy;Copyright for [Shuang Wu] [2017]<br>\n",
    "Cite from the [coursera] named [Neural network and Machine Learning] from [deeplearning.ai]<br>\n",
    "Learning notes<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch gradient descent\n",
    "\n",
    "* Batch vs. mini-batch gradient descent\n",
    "    * Vectorization allows to efficiently compute on $m$ examples\n",
    "        * $X=[x_1, x_2, \\cdots, x_m]$\n",
    "        * $Y=[y_1, y_2, \\cdots, y_m]$\n",
    "        * but when $m$ is really large, the process will be very slow\n",
    "    * then can set mini-bathes of 1000 each\n",
    "        * $X^{\\{1\\}}=[x_1, x_2, \\cdots, x_{1000}]$\n",
    "        * $X^{\\{2\\}}=[x_{1001}, x_{1002}, \\cdots, x_{2000}]$\n",
    "        * $Y^{\\{1\\}}=[y_1, y_2, \\cdots, y_{1000}]$\n",
    "        * $Y^{\\{2\\}}=[y_{1001}, y_{1002}, \\cdots, y_{2000}]$\n",
    "        * mini-batch t: $X^{\\{t\\}}$ , $Y^{\\{t\\}}$\n",
    "     \n",
    "* Mini-batch gradient descent\n",
    "    * ```python\n",
    "    for t =1,..., 5000\n",
    "    ```\n",
    "    * forward prop and compute the cost\n",
    "    * backprop and update the weights\n",
    "    * this is \"1 epoch\" (single pass to the training set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training w/ mini batch gradient descent\n",
    "\n",
    "* For batch gradient descent\n",
    "    * ![img1](imgs/img1.jpg)\n",
    "* Mini-batch gradient descent\n",
    "    * ![img2](imgs/img2.jpg)\n",
    "* Choosing your mini-batch size\n",
    "    * if = m : batch gradient descent\n",
    "        * too long per iteration\n",
    "    * if = 1: stochastic gradient descent\n",
    "        * each e.g. in it own mini-batch\n",
    "        * lose speed from vectorization\n",
    "    * In practice, use some size between 1 and m\n",
    "        * Fastest learning\n",
    "        * vectorization on (~1000)\n",
    "        * making progress w/0 process entire training set\n",
    "    * ![img3](imgs/img3.jpg)\n",
    "        * purpal for Stochastic-GD\n",
    "        * Blue for Batch-GD\n",
    "* Chossing mini-batch size\n",
    "    * if small train set: use batch GD (m<=2000)\n",
    "    * Typical mini-batch size: $2^6$, $2^7$, $2^8$, $2^9$, $2^{10}$\n",
    "    * Make sure minibatch fit in CPU/GPU memory\n",
    "* try different variable and see the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponentially weighted averages\n",
    "\n",
    "* Temperature in London\n",
    "    * ![img4](imgs/img4.jpg)\n",
    "    * ![img5](imgs/img5.jpg)\n",
    "    * $\\beta=0.5$ : $\\approx$ 2 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponentially weighted averages\n",
    "\n",
    "* Exponentially weighted averages\n",
    "    * $v_t = \\beta v_{t-1}+(1-\\beta)\\theta_t$\n",
    "    * $v_t = (1-\\beta)\\theta_t + (1-\\beta)*\\beta^{1}\\theta_{t-1} + (1-\\beta)*\\beta^{2}\\theta_{t-2} + \\cdots + (1-\\beta)*\\beta^{t-1}\\theta_{1}$\n",
    "    * $(1-\\epsilon)^{\\frac{1}{\\epsilon}}=\\frac{1}{e}$\n",
    "    \n",
    "* Implementing exponentially weighted averages\n",
    "    * ```\n",
    "    v = 0\n",
    "    repeat{\n",
    "        get next theta\n",
    "        v := beta*v + (1-beta)*theta\n",
    "        }\n",
    "    ```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias correction in exponentially weighted averages\n",
    "\n",
    "* Bias correction\n",
    "    * instead of using vt directly using the following formula\n",
    "    * $\\frac{v_t}{1-\\beta^t}$\n",
    "        * the green line instead of the purpul line\n",
    "    * ![img6](imgs/img6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient descent w/ momentum\n",
    "\n",
    "* GD e.g.\n",
    "    * want slower learning for horizontal and faster for vertical\n",
    "        * ![img7](imgs/img7.jpg)\n",
    "    * use Momentum\n",
    "        * ![img8](imgs/img8.jpg)\n",
    "    * for the bowl shape, the derivative term can be seen as accelaration and the momentum term can be seen as velocity\n",
    "\n",
    "* Implementation details\n",
    "    * On iteration t:\n",
    "        * compute dW, db on the current mini-batch\n",
    "        * $v_{dW}=\\beta v_{dW}+(1-\\beta)dW$\n",
    "        * $v_{db}=\\beta v_{db}+(1-\\beta)db$\n",
    "        * $W = W-\\alpha v_{dW}$, $b = b-\\alpha v_{db}$ \n",
    "    * Hyperparameters: $\\alpha$, $\\beta$\n",
    "        * $\\alpha=0.9$\n",
    "            * average over last $\\approx 10$ gradients\n",
    "        * in practice people do not use bias correction\n",
    "        * people may do not include the $(1-\\beta)$, because the learning rate will also affect this one, but prefer included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop (Root mean square prop)\n",
    "\n",
    "* RMSprop\n",
    "    * ![img9](imgs/img9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam optimization algorithm\n",
    "\n",
    "* Adam optimization algo.\n",
    "    * initialize $V_{dw}=0$, $S_{dw}=0$, $V_{db}=0$, $S_{db}=0$\n",
    "    * on iteration t:\n",
    "        * compute $dw$, $db$ using current mini-batch\n",
    "        * $V_{dw} = \\beta_1 V_{dw} + (1-\\beta_1)dw$, $V_{db} = \\beta_1 V_{db} + (1-\\beta_1)db$\n",
    "        * $S_{dw} = \\beta_2 S_{dw} + (1-\\beta_2)dw^2$, $S_{db} = \\beta_2 S_{db} + (1-\\beta_2)db^2$\n",
    "            * $\\beta_1$ for momentum\n",
    "            * $\\beta_2$ for RMSprop\n",
    "        * $V^{correct}_{dw} = V_{dw}/(1-\\beta_1^t)$, $V^{correct}_{db} = V_{db}/(1-\\beta_1^t)$\n",
    "        * $S^{correct}_{dw} = S_{dw}/(1-\\beta_2^t)$, $S^{correct}_{db} = S_{db}/(1-\\beta_2^t)$\n",
    "        * $$w := w-\\alpha\\frac{V^{correct}_{dw}}{\\sqrt{S^{correct}_{dw}}+\\epsilon}$$\n",
    "        * $$b := b-\\alpha\\frac{V^{correct}_{db}}{\\sqrt{S^{correct}_{db}}+\\epsilon}$$\n",
    "        \n",
    "* Hyperparameters choice:\n",
    "    * $\\alpha$: need to be tune\n",
    "    * $\\beta_1$: 0.9 ($dw$)\n",
    "    * $\\beta_2$: 0.999 ($dw^2$)\n",
    "    * $\\epsilon$: $10^{-8}$\n",
    "\n",
    "* Adam\n",
    "    * adaptive moment estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate decay\n",
    "\n",
    "* Learning rate decay\n",
    "    * Initial step learning can take bigger faster step, but when approch convergence, need slower the step\n",
    "    * ![img10](imgs/img10.jpg)\n",
    "        \n",
    "    * 1 epoch = 1 pass through train\n",
    "    * ![img11](imgs/img11.jpg)\n",
    "    * $$\\alpha = \\frac{1}{1+(decay-rate)* (epoch-num)}\\alpha_0$$\n",
    "    \n",
    "* Other learning rate decay method.\n",
    "    * $\\alpha = 0.95^{epoch-num}\\alpha_0$, exponentially decay\n",
    "    * $\\alpha = \\frac{k}{\\sqrt{epoch-num}}\\alpha_0$ or $\\alpha = \\frac{k}{\\sqrt{t}}\\alpha_0$\n",
    "    * manual decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem of local optima\n",
    "\n",
    "* Local optima in neural networks\n",
    "    * ![img12](imgs/img12.jpg)\n",
    "    * unlikely to get stuck in a bac local optima\n",
    "    * plateaus can make learning slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
