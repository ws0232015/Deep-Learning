{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy;Copyright for [Shuang Wu] [2017]<br>\n",
    "Cite from the [coursera] named [Neural network and Machine Learning] from [deeplearning.ai]<br>\n",
    "Learning notes<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up your ML app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Dev / Test sets\n",
    "\n",
    "* Iterative process to find:\n",
    "    * #of layers\n",
    "    * #of hidden units\n",
    "    * learning rates\n",
    "    * activation\n",
    "    * etc.\n",
    "    \n",
    "* Train / Dev / Test sets\n",
    "    * Training set\n",
    "        * pre: 70%/60%\n",
    "        * big data: 98%/99.5%\n",
    "    * Hold-out cros validation set (Development set)('dev')\n",
    "        * pre: 20%\n",
    "        * big data: 1%\n",
    "    * Test set\n",
    "        * pre: 30%/20%\n",
    "        * big data: 1%\n",
    "\n",
    "* Mismatched train/test distr.\n",
    "    * training set:\n",
    "        * cat picturec from webpages\n",
    "    * Dev/test sets:\n",
    "        * cat pictures from users using your app\n",
    "    * then ditr. from this two set maybe different\n",
    "    * Make sure dev and test come from same distribution\n",
    "    * Not having a test set might be okay. (Only dev set.)\n",
    "    \n",
    "## Bias and Variance\n",
    "\n",
    "* Hight bias\n",
    "    * underfitting\n",
    "        * Train set error: 15%\n",
    "        * Dec set error:16%\n",
    "* Just right\n",
    "    * Train set error: 0.5%\n",
    "    * Dec set error:1%\n",
    "* High variance\n",
    "    * overfitting\n",
    "        * Train set error: 1%\n",
    "        * Dec set error:11%\n",
    "* Both high bais & variance:\n",
    "    * Train set error: 15%\n",
    "    * Dec set error:30%\n",
    "* For human, optimal (Bayes) error: 15%\n",
    "    * Blury image\n",
    "    \n",
    "## Basic \"recipe\" for ML\n",
    "\n",
    "* If High bias (training data performance)\n",
    "    * Try Bigger network\n",
    "    * training longer\n",
    "    * NN architecture search\n",
    "* If High variance ( dev set performance)\n",
    "    * try more data\n",
    "    * try regularization\n",
    "    * NN arch. search\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Regularizing NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "* logistic regression\n",
    "    * L2 regularization\n",
    "        * $+\\frac{\\lambda}{2m}\\|w\\|^2_2$\n",
    "        * $\\|w\\|^2_2 = \\sum_{j=1}^{n_x}w_j^2=w^Tw$\n",
    "    * L1 regularization\n",
    "        * $\\frac{\\lambda}{2m}\\sum_{j=1}^{n_x}|w_j|=\\frac{\\lambda}{2m}\\|w\\|_1$\n",
    "    * Lamda\n",
    "        * regularization parameter\n",
    "\n",
    "* NN\n",
    "    * $+\\frac{\\lambda}{2m}\\sum^L_{l=1}\\|w^{[l]}\\|^2_F$\n",
    "    * Forbenius norm for matrix: \n",
    "        * $\\|w^{[l]}\\|^2_F=\\sum_{i=1}^{n^{[l-1]}}\\sum_{j=1}^{n^{[l]}}(w_{ij}^{[l]})^2$\n",
    "    * GD:\n",
    "        * $dw^{[l]} = \\text{(from backprop)} + \\frac{\\lambda}{m}w^{[l]}$\n",
    "        * $w^{[l]} := w^{[l]} - \\alpha dw^{[l]}$\n",
    "        * $\\frac{\\partial J}{\\partial w^{[l]}}= dw^{[l]}$\n",
    "        \n",
    "## Why regularizationg reduces overfitting\n",
    "\n",
    "* when $lambda$ is really large, may set those w close to zero, then zeroing out some hidden neurons which can from high variacne to high bias\n",
    "    * smaller network\n",
    "* when lambda large, the weights will be relative small, smake as z, then every layer will approx to linear and the whole NN will be near liner \n",
    "\n",
    "## Dropout regularization\n",
    "\n",
    "* Dropout Regularization\n",
    "    * Go through each layer, and set the probability of total numbers of neurons to 0\n",
    "    * Repeat this process\n",
    "* Implementing dropout (\"Inverted dropout\")\n",
    "    * Illustrate w/ layer 3\n",
    "        * keep_prob = 0.8\n",
    "        * d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep.prob\n",
    "        * a3 = np.multply(a3, d3)\n",
    "        * a3 /= keep_prob\n",
    "            * Ensure the expected value of a3 remain the same\n",
    "* Making predictions at test time\n",
    "    * no drop out\n",
    "    \n",
    "## Understanding Dropout\n",
    "\n",
    "* Why dropout work?\n",
    "    * Intuition: can't rely on any one feature, so have to spread out weights\n",
    "        * shrink weights (L2)\n",
    "* Can have low keep-prob for layer have lots of neurons which may cause overfitting, vice versa.\n",
    "* good in compute vision\n",
    "* Dropout is use for regularization, for overfitting\n",
    "\n",
    "## Other regularization methods\n",
    "\n",
    "* data augmentation\n",
    "    * flippling image horizontally\n",
    "    * rotation, cut, zoom in\n",
    "* Early stopping\n",
    "    * ![img1](imgs/img1.jpg)\n",
    "* Orthogonalization\n",
    "    * Optimize cost func J\n",
    "        * GD\n",
    "    * Not overfit\n",
    "        * Regularization\n",
    "        * more data\n",
    "    * these two can not independently "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Setting up optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizaing inputs\n",
    "\n",
    "* 2-step\n",
    "    * subtract mean\n",
    "        * $\\mu=\\frac{1}{m}\\sum^m_{i=1}x^{(i)}$\n",
    "        * $x:=x-\\mu$\n",
    "    * Normalize variance\n",
    "        * $\\sigma^2=\\frac{1}{m}\\sum^m_{i=1}(x^{(i)})^2$\n",
    "        * $x /= \\sigma^2$\n",
    "* ![img2](imgs/img2.jpg)\n",
    "* Use the same $\\mu$ and $\\sigma^2$ from training set to normalize the test set\n",
    "\n",
    "* why normalize inputs\n",
    "    * range of the value make the elongate bowl or elongate func.\n",
    "    * normalize will solve this problem and only need a  small learning rate\n",
    "    * ![img3](imgs/img3.jpg)\n",
    "    * easier and faster to optimize\n",
    "    \n",
    "* Can always do this\n",
    "\n",
    "## Vanishing / Exploding gradients\n",
    "\n",
    "## Weight initialization for Deep nets.\n",
    "\n",
    "* Single neuron e.g.\n",
    "    * ReLU\n",
    "        * $w^l=np.random.randn(shape)*np.sqrt(\\frac{2}{n^{[l-1]}})$\n",
    "    * large n $\\rightarrow$ smaller $w_i$\n",
    "\n",
    "## Numerical approxm. of gradients\n",
    "\n",
    "* Checking derivative computation\n",
    "    * ![img4](imgs/img4.jpg)\n",
    "        * $\\frac{f(\\theta+\\epsilon)-f(\\theta-\\epsilon)}{2\\epsilon}\\approx g(\\theta)$\n",
    "        * Approx error: 0.0001\n",
    "\n",
    "## Gradient checking\n",
    "\n",
    "* For NN\n",
    "    * Take W, b and reshape into a big vector $\\theta$\n",
    "    * Take dW, db and reshape into a big vector $d\\theta$\n",
    "    * for each i:\n",
    "        * $d\\theta_{approx}[i]= (J(\\theta_1,\\theta_2,\\cdots,\\theta_i+\\epsilon,\\cdots)-J(\\theta_1,\\theta_2,\\cdots,\\theta_i-\\epsilon,\\cdots))/2\\epsilon$\n",
    "        * $\\approx d\\theta[i]=\\frac{\\partial J}{\\partial \\theta_i}$\n",
    "        * check $$\\frac{\\|d\\theta_{approx}-d\\theta\\|_2}{\\|d\\theta_{approx}\\|_2+\\|d\\theta\\|_2}$$\n",
    "            * $\\approx 10^{-7}$ great\n",
    "            * $\\approx 10^{-5}$ \n",
    "            * $\\approx 10^{-3}$ worry\n",
    "            \n",
    "## Gradient checking implementating notes\n",
    "\n",
    "* Don't use in training - only to debug\n",
    "* If algo. fails grad check, look at components to tro to identify bug\n",
    "* Remember regularization\n",
    "* Doesn't work w/ dropout\n",
    "* Run at random initialization; perhaps again after some training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
