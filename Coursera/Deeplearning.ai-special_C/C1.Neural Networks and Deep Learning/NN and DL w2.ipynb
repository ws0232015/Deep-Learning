{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy;Copyright for [Shuang Wu] [2017]<br>\n",
    "Cite from the [coursera] named [Neural network and Machine Learning] from [deeplearning.ai]<br>\n",
    "Learning notes<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression as a NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "* Binary Classification\n",
    "    * lagel the img is cat (1) vs non-cat (0) (y)\n",
    "        * img is in as red, green, blue intensities\n",
    "        * each pixel is a value, e.g. $64*64$\n",
    "        * x is just list all these pixeles in a single col.\n",
    "        * x can also be $64*64*3=12288$\n",
    "        * $n_x=12288$\n",
    "        * want to predict $x\\rightarrow y$\n",
    "        ![img12](imgs/img12.jpg)\n",
    "* Notation\n",
    "    * $(x,y)$ single example\n",
    "        * $x\\in\\mathbb{R}^{n_x}$\n",
    "        * $y\\in{0,1}$\n",
    "    * m training example: {($x^1$, $y^1$), ($x^2$, $y^2$),$\\cdots$,($x^m$, $y^m$)}\n",
    "        * $m=m_{train}$\n",
    "        * $m=m_{test}$\n",
    "    * X = $[\\vec{x^1}, \\vec{x^2}, \\cdots, \\vec{x^m},]$\n",
    "        * $X\\in\\mathbb{R}^{n_x*m}$\n",
    "        * X.shape=($n_x$,m)\n",
    "        ![img13](imgs/img13.jpg)\n",
    "    * Y = [$y^1$, $y^2$, $\\cdots$, $y^m$]\n",
    "        * $Y\\in\\mathbb{R}^{1*m}$\n",
    "        * Y.shape = (1,m)\n",
    "        ![img14](imgs/img14.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Reg.\n",
    "* Logistic regression is a learning algorithm used in supervised learning problem\n",
    "    * the output y are all 0 or 1\n",
    "    * goal is to minimize the error b/w predictions and training data\n",
    "* E.g.: Cat vs No-cat\n",
    "    * Given an image represented by feature vector $x$\n",
    "    * Probability of cat in img:\n",
    "        * $Given\\quad x, \\hat{y}=P(y=1|x)\\quad, where\\quad0\\leq\\hat{y}\\leq1$\n",
    "    * Parameters in the logistic reg:\n",
    "        * input fectures vecotr: $x\\in\\mathbb{R}^{n_x}$\n",
    "        * training label: y\n",
    "        * weights: $w\\in\\mathbb{R}^{n_x}$\n",
    "        * threshold: $b\\in\\mathbb{R}$\n",
    "        * output $\\hat{y}=\\sigma(w^Tx+b) = \\sigma(z)$\n",
    "        * sigmoid function: $s=\\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
    "        ![img15](imgs/img15.jpg)\n",
    "        * $\\sigma(w^Tx+b)$ is a linear function $(ax+b)$, but we are looking for prob. constraint b/w [0,1], so use sigmoid func.\n",
    "        * Obs. from this graph\n",
    "            * bounded b/w [0,1]\n",
    "            * if z large positive, $\\sigma(z)=1$\n",
    "            * if z small negative, $\\sigma(z)=0$\n",
    "            * if $z=0$, $\\sigma(z)=0.5$\n",
    "* It's easy to use b and w separately instead of using the $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Reg. Cost function\n",
    "\n",
    "* Loss (error) function:\n",
    "    * Measure the discrepancy b/t predication ($\\hat{y}^{(i)}$) and desired output ($y^{(i)}$)\n",
    "    * computes the error for single training example\n",
    "    * $L(\\hat{y}^{(i)}, y^{(i)}) = \\frac{1}{2}(\\hat{y}^{(i)}-y^{(i)})^2$\n",
    "        * not convex so do not use this, because the GD will not find the global minimization\n",
    "    * $L(\\hat{y}^{(i)}, y^{(i)}) = -(y^{(i)}log(\\hat{y}^{(i)}) + (1-y^{(i)})log(1-\\hat{y}^{(i)}))$<br>\n",
    "        * this will make the convex\n",
    "    <br>\n",
    "        * If $y^{(i)}=1$: $L(\\hat{y}^{(i)}, y^{(i)})=-log(\\hat{y}^{(i)})$ where $log(\\hat{y}^{(i)})$ and $\\hat{y}^{(i)}$ should be close to 1\n",
    "        * If $y^{(i)}=0$: $L(\\hat{y}^{(i)}, y^{(i)})=-log(1-\\hat{y}^{(i)})$ where $log(1-\\hat{y}^{(i)})$ and $\\hat{y}^{(i)}$ should be close to 0<br>\n",
    "        <br>\n",
    "* Cost function\n",
    "    * To train para. w and b\n",
    "    * The average of the loss func. of the entire training set\n",
    "    * want the para. w and b that <font color='red'>minimize</font> the overall cost func.\n",
    "    $$J(w,b)=\\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)}, y^{(i)})=-\\frac{1}{m}\\sum^m_{i=1}[(y^{(i)}log(\\hat{y}^{(i)}) + (1-y^{(i)})log(1-\\hat{y}^{(i)}))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "* GD\n",
    "    * Convex\n",
    "    ![img16](imgs/img16.jpg)\n",
    "    * non-convec\n",
    "    ![img17](imgs/img17.jpg)\n",
    "    ![img18](imgs/img18.jpg)\n",
    "    * initialize like the top red dot\n",
    "    * then take step down hill in the direction\n",
    "    * repeat the process until converge tor close to the global optimal\n",
    "    * Process\n",
    "        * repeat do {w:= w-$\\alpha\\frac{dJ(w)}{dw}$}\n",
    "            * $\\alpha$ is the learning rate\n",
    "            * $w:=w-\\alpha dw$\n",
    "            * the derivative is the slope\n",
    "        ![img18](imgs/img19.jpg)\n",
    "        * $J(w, b)$ \n",
    "            * $w:=w-\\alpha\\frac{\\partial J(w,b)}{\\partial w}$\n",
    "                * $w:=w-\\alpha\\partial w$\n",
    "            * $b:=b-\\alpha\\frac{\\partial J(w,b)}{\\partial b}$\n",
    "                * $b:=b-\\alpha\\partial b$\n",
    "            * use $\\partial$ when the function have at least two variable\n",
    "        * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives\n",
    "\n",
    "* Intuition\n",
    "    * $f(a)=3a$\n",
    "    ![img20](imgs/img20.jpg)\n",
    "        * a=2, f(a)=6\n",
    "            * a=2.001, f(a)=6.003\n",
    "            * increase the $a$ by 0.001, same as increase the $f(a)$ by 0.003\n",
    "            * slope (derivative) of $f(a)$ at $a=2$ is 3\n",
    "            * slope os the height divided by the width\n",
    "        * a=5, f(a)=15\n",
    "            * a=5.001, f(a)=15.003\n",
    "            * slope (derivative) of $f(a)$ at $a=5$ is 3\n",
    "        * $\\frac{df(a)}{da}=3=\\frac{d}{da}f(a)$\n",
    "        * stright line have the same slope everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Derivatives\n",
    "\n",
    "* Intuition\n",
    "    * $f(a)=a^2$\n",
    "    ![img21](imgs/img21.jpg)\n",
    "        * a=2, f(a)=4\n",
    "            * a = 2.001, $f(a)\\approx 4.004$\n",
    "            * slope (derivative) of f(a) at a=2 is 4\n",
    "            * $\\frac{d}{da}f(a)=4$ when a =2\n",
    "        * a=5, f(a)=25\n",
    "            * a =5.001, $f(a)\\approx 25.010$\n",
    "            * slope (derivative) of f(a) at a=5 is 10\n",
    "            * $\\frac{d}{da}f(a)=10$ when a =5\n",
    "        * $\\frac{d}{da}f(a)=\\frac{d}{da}a^2=2a$\n",
    "* more e.g.\n",
    "    * $f(a)=a^3$\n",
    "        * $\\frac{d}{da}f(a)=3a^2$\n",
    "    * $f(a)=log(a)$\n",
    "        * $\\frac{d}{da}f(a)=\\frac{1}{a}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation graph\n",
    "\n",
    "* J(a,b,c) = 3(a + bc)\n",
    "    * u=bc\n",
    "    * v =a + u\n",
    "    * J = 3*v\n",
    "    ![img22](imgs/img22.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives w/ a computation graph\n",
    "\n",
    "* one step back to calculate the derivative\n",
    "* Chain rule\n",
    "    * $\\frac{dJ}{da}= \\frac{dJ}{dv}\\frac{dv}{da}$\n",
    "    ![img23](imgs/img23.jpg)\n",
    "* Final Output Var.\n",
    "    * $\\frac{d FOV}{d Var}$\n",
    "        * \"d var\"\n",
    "    ![img24](imgs/img24.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression Gradient Descent\n",
    "\n",
    "* ![img25](imgs/img25.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GD on m Examples\n",
    "\n",
    "* use for loop and then divided by m\n",
    "* ![img26](imgs/img26.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "* Vectorization\n",
    "    * $z=w^Tx+b$\n",
    "        * w and x are both vectors $\\in\\mathbb{R}^{n_x}$\n",
    "* GPU and CPU\n",
    "    * SIMD - single instantian multiple data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# non-vec cal\n",
    "z=0\n",
    "for i in range(n):\n",
    "    z+= w[i]*x[i]\n",
    "    z+=b\n",
    "    \n",
    "## vec cal\n",
    "z = np.dot(w,x) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250146.759712\n",
      "Vectorized version: 131.99996948242188ms\n",
      "250146.759712\n",
      "For loop: 868.9999580383301ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "a=np.array([1,2,3,4])\n",
    "\n",
    "a=np.random.rand(1000000)\n",
    "b=np.random.rand(1000000)\n",
    "\n",
    "tic = time.time()\n",
    "c = np.dot(a,b)\n",
    "toc=time.time()\n",
    "\n",
    "print(c)\n",
    "print(\"Vectorized version: \" + str(1000*(toc-tic))+\"ms\")\n",
    "\n",
    "c=0\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c += a[i]*b[i]\n",
    "toc = time.time()\n",
    "\n",
    "print(c)\n",
    "print(\"For loop: \" + str(1000*(toc-tic))+\"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More vectorization\n",
    "\n",
    "* when possible, avoid for-loops\n",
    "    * apply the exp. operation on every element of a matrix/vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-333de0428862>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# no-vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "# no-vec\n",
    "u = np.zeros(n,1)\n",
    "for i in range(n):\n",
    "    u[i]=math.exp(v[i])\n",
    "    \n",
    "# vec\n",
    "u = np.exp(v)\n",
    "\n",
    "np.log(v)\n",
    "np.abs(v)\n",
    "np.maximum(v, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Logistic regression\n",
    "\n",
    "* Broadcasting\n",
    "* use vectorization to reduce the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vec. logsitic regression gradient output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# db=1/ m sum(dz(i))\n",
    "1/m*(np.sum(dz))\n",
    "\n",
    "#dw = 1/m Xdz.T\n",
    "1/m*(np.sum(np.dot(X,dz)))\n",
    "\n",
    "# w and b cal\n",
    "z = np.dot(w.T, X)+b\n",
    "A = simgma(z)\n",
    "dz = A*Y\n",
    "dw = 1/m*X*dZ.T\n",
    "db=1/m*np.sum(dz)\n",
    "w -= alpha*dw\n",
    "b -= alpha*db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting in Python\n",
    "![img27](imgs/img27.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  56.     0.     4.4   68. ]\n",
      " [   1.2  104.    52.     8. ]\n",
      " [   1.8  135.    99.     0.9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[56.0, 0.0, 4.4, 68.0],\n",
    "             [1.2, 104.0, 52.0, 8.0],\n",
    "             [1.8, 135.0, 99.0, 0.9]])\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  59.   239.   155.4   76.9]\n"
     ]
    }
   ],
   "source": [
    "cal = A.sum(axis=0)\n",
    "print(cal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on python/numpy vec.\n",
    "\n",
    "* do not use rank-one array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.70027017 -1.150852   -0.19721797  1.19697947 -3.11958897]\n",
      "(5,)\n",
      "[ 1.70027017 -1.150852   -0.19721797  1.19697947 -3.11958897]\n",
      "15.4188691009\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.randn(5)\n",
    "\n",
    "print(a) # rank-1\n",
    "print(a.shape)\n",
    "print(a.T)\n",
    "print(np.dot(a, a.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.05841501]\n",
      " [ 0.98644671]\n",
      " [ 1.19374008]\n",
      " [ 0.30440944]\n",
      " [ 1.52607674]]\n",
      "[[-1.05841501  0.98644671  1.19374008  0.30440944  1.52607674]]\n",
      "[[ 1.12024233 -1.04407    -1.26347242 -0.32219152 -1.61522252]\n",
      " [-1.04407     0.97307711  1.17756098  0.30028369  1.50539338]\n",
      " [-1.26347242  1.17756098  1.42501538  0.36338575  1.82173897]\n",
      " [-0.32219152  0.30028369  0.36338575  0.09266511  0.46455217]\n",
      " [-1.61522252  1.50539338  1.82173897  0.46455217  2.32891021]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(5, 1)\n",
    "print(a)\n",
    "print(a.T)\n",
    "print(np.dot(a, a.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick tour of Jupyter/ipy nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanationof logistic regression cost func.\n",
    "\n",
    "* if y=1: $p(y|x)=\\hat{y}$\n",
    "* if y=0: $p(y|x)=1-\\hat{y}$\n",
    "* $p(y|x) = \\hat{y}^y(1-\\hat{y})^{(1-y)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.random.randn(12288, 150) # a.shape = (12288, 150)\n",
    "b = np.random.randn(150, 45) # b.shape = (150, 45)\n",
    "c = np.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12288, 45)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,3) (3,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4bd2cd4ebeba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# a.shape = (4, 3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# b.shape = (3, 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,3) (3,2) "
     ]
    }
   ],
   "source": [
    "a = np.random.randn(4, 3) # a.shape = (4, 3)\n",
    "b = np.random.randn(3, 2) # b.shape = (3, 2)\n",
    "c = a*b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
