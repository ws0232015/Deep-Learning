{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy;Copyright for [Shuang Wu] [2017]<br>\n",
    "Cite from the [coursera] named [Neural network and Machine Learning] from [deeplearning.ai]<br>\n",
    "Learning notes<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Overview\n",
    "\n",
    "* Waht is a NN?\n",
    "    * Str\n",
    "        * ![img28](imgs/img28.jpg)\n",
    "    * w/ back propagation to calculate dz and da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Representation\n",
    "\n",
    "* ![img29](imgs/img29.jpg)\n",
    "    * Input Layer\n",
    "        * $a^{[0]}=x$\n",
    "            * a: activations\n",
    "    * Hidden layer\n",
    "        * $a^{[1]}$=\n",
    "            * [$a^{[1]}_1$, $a^{[1]}_2$, $a^{[1]}_3$, $a^{[1]}_4$]\n",
    "        * $w^{[1]}$\n",
    "            * (4,3)\n",
    "        * $b^{[1]}$\n",
    "            * (4,1)\n",
    "    * Output Lyaer\n",
    "        * $a^{[2]}= \\hat{y}$\n",
    "        * $w^{[2]}$\n",
    "            * (1,4)\n",
    "        * $b^{[2]}$\n",
    "            * (1,1)\n",
    "    * 2 layer NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing a NN's Output\n",
    "\n",
    "* $z=w^Tx + b$\n",
    "* $a=\\sigma(z)$\n",
    "* $a^{[l]}_i$\n",
    "    * l is the layer\n",
    "    * i is the node in layer\n",
    "    * ![img30](imgs/img30.jpg)\n",
    "* $Z_1^{[1]}=w_1^{[1]T}x+b_1^{[1]}$\n",
    "    * $a_1^{[1]}=\\sigma(z_1^{[1]})$\n",
    "* $Z_2^{[1]}=w_2^{[1]T}x+b_2^{[1]}$\n",
    "    * $a_2^{[1]}=\\sigma(z_2^{[1]})$\n",
    "* $Z_3^{[1]}=w_3^{[1]T}x+b_3^{[1]}$\n",
    "    * $a_3^{[1]}=\\sigma(z_3^{[1]})$\n",
    "    \n",
    "* $Z_4^{[1]}=w_4^{[1]T}x+b_4^{[1]}$\n",
    "    * $a_4^{[1]}=\\sigma(z_4^{[1]})$\n",
    "* Vectorize\n",
    "    * ![img31](imgs/img31.jpg)\n",
    "    \n",
    "* Given input x:\n",
    "    * $z^1 = W^1 +b^1$\n",
    "    * $a^1 = \\sigma(z^1)$\n",
    "    * $z^2 = W^2a^1 +b^2$\n",
    "    * $a^2 = \\sigma(z^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing across multiple e.g.\n",
    "\n",
    "* for loop\n",
    "    * for i=1 to m:<br>\n",
    "        $z^{[1](i)}=W^{[1]}x^{(i)}+b^{[1]}$<br>\n",
    "        $a^{[1](i)}=\\sigma(z^{[1](i)})$<br>\n",
    "        $z^{[2](i)}=W^{[2]}a^{[1](i)}+b^{[2]}$<br>\n",
    "        $a^{[2](i)}=\\sigma(z^{[2](i)})$<br>\n",
    "    * ![img32](imgs/img32.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation for vectorized implementation\n",
    "\n",
    "* Justification for vectorized implementation\n",
    "    * ![img33](imgs/img33.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Activation functions\n",
    "\n",
    "* Sigmoid function is one of the activation func.\n",
    "    * $a=\\frac{1}{1+e^{-z}}=\\sigma(z)$\n",
    "    * ![img34](imgs/img34.jpg)\n",
    "* In general\n",
    "    * $a = g(z)$\n",
    "    * $a = tanh(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}$\n",
    "        * center the data\n",
    "        * almost 0 mean\n",
    "        * better than sigmoid\n",
    "        * ![img35](imgs/img35.jpg)\n",
    "* when binary classification, may use sigmoid\n",
    "* when z very large or very small, the derivative very small and can slow down the GD\n",
    "* activation can be different for diff. layer\n",
    "* ReLU\n",
    "    * ![img36](imgs/img36.jpg)\n",
    "    * $a = max(0, z)$\n",
    "    * slope is 1 when positive and 0 when negative\n",
    "    * when exactly equal 0, is not well define\n",
    "* Rule for choosing func.\n",
    "    * binary classification {0,1}\n",
    "        * sigmoid\n",
    "    * all other units\n",
    "        * ReLU (rectified linear unit)\n",
    "* LReLU\n",
    "    * Leaky ReLU\n",
    "    * $a = max(0.01z, z)$\n",
    "     * ![img37](imgs/img37.jpg)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we need non-linear activations func.\n",
    "\n",
    "* If we do linear activation\n",
    "    * $a^1 = z^1 = w^1x+b^1$\n",
    "    * $a^2 = z^2 = w^2a^1+b^2 = w^2(w^1x+b^1)+b^2 = w^2w^1x+w^2b^1+b^2=w'x+b'$\n",
    "* composition of linear is linear\n",
    "* Only when doing **regression** problem can using the linear activations on the **last layer (output layer)** not the hidden layer\n",
    "    * but the hose price problem can not got negative value, so ReLU better than linear activations func."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivatives of activation func.\n",
    "\n",
    "* Sigmoid act. func.\n",
    "    * $g(z)=\\frac{1}{1+e^{-z}}$\n",
    "    * $g'(z) = \\frac{d}{dz}g(z) = $ slope of g(x) at z\n",
    "        * $=\\frac{1}{1+e^{-z}}(1-\\frac{1}{1+e^{-z}})=g(z)(1-g(z))$\n",
    "        * $=a(1-a)$\n",
    "        \n",
    "* Tanh act. func.\n",
    "    * $g(z)=tanh(z) = \\frac{e^z-e^{-z}}{e^z+e^{-z}}$\n",
    "    * $g'(z) = \\frac{d}{dz}g(z) = $ slope of g(x) at z\n",
    "        * $=1-(tanh(z))^2$\n",
    "        * $=1-a^2$ for $a=g(z)$\n",
    "\n",
    "* ReLU and Leaky ReLU\n",
    "    * ReLU\n",
    "        * $g(z) = max(0, z)$\n",
    "        * $g'(z) = \\begin{cases} 0& if& z<0\\\\\n",
    "        1& if& z\\geq 0 \\\\\\end{cases}$\n",
    "    * LReLU\n",
    "        * $g(z) = max(0.01z, z)$\n",
    "        * $g'(z) = \\begin{cases} 0.01& if& z<0\\\\\n",
    "        1& if& z\\geq 0 \\\\\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GD for NN\n",
    "\n",
    "* equations\n",
    "    * ![img38](imgs/img38.jpg)\n",
    "* Formulas for computing derivatives\n",
    "    * Forward propagation:\n",
    "        * $z^1=w^1x+b^1$\n",
    "        * $A^1=g^1z^1$\n",
    "        * $z^2=w^2A^1+b^2$\n",
    "        * $A^2=g^2z^2$\n",
    "    * Back propagation\n",
    "        * ![img39](imgs/img39.jpg)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Intuition\n",
    "\n",
    "* For logistic\n",
    "    * ![img40](imgs/img40.jpg)\n",
    "* back propag.\n",
    "    * ![img41](imgs/img41.jpg)\n",
    "* Summary\n",
    "    * ![img42](imgs/img42.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Initialization\n",
    "\n",
    "* Initialize zero ok for logistic but not for NN\n",
    "    * because this will cause the neuron to be computing the exactly same thing, or symmatric\n",
    "    * cause the hidden layer symmetric which cause all calculate the same thing\n",
    "* Random init.\n",
    "    * $w = np.random.rand([2,2])*0.01$\n",
    "        * if use 100 instead of 0.01, then weh ncalculate the activations, it will slow down the GD, or the slope will be very small\n",
    "    * $b = np.zero((2,1))$\n",
    "        * b is ok even init. as 0 becuase do not have those symmetric problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
