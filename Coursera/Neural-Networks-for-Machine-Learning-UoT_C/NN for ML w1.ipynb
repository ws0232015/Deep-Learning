{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&copy;Copyright for [Shuang Wu] [2017]<br>\n",
    "Cite from the [coursera] named [Neural network for Machine Learning] from [University of Toronto]<br>\n",
    "Learning notes<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need machine learning?\n",
    "\n",
    "#### What is ML?<br>\n",
    "1. hard to write programs that can recongnizing 3-D object from a novel viewpoint in new lighting condistions in a cluttered scene.<br>\n",
    "    (1). Don't know neither how to write this program nor how brain work with this problem.<br>\n",
    "    (2). Program maybe horrendously complicated.<br>\n",
    "2. hard to write program to compute the probability that a credit card fraudulent transaction.<br>\n",
    "    (1). no simple or reliable rule and need combine large # of weak rules.<br>\n",
    "    (2). Program need keep changing to follow the fraud.<br>\n",
    "        \n",
    "#### The ML Approach<br>\n",
    "1. Instead write program for each task, collecting lots of examples that specift the correct output for given input.<br>\n",
    "2. ML algorithm takes the problem and produces program that does the job<br>\n",
    "    (1). Program produced by ML may different from hand-written program. Containning millions of #s.<br>\n",
    "    (2). If right, program works for both new cases and the training one.<br>\n",
    "    (3). Data change, program change, training on new data.<br>\n",
    "3. Massive amounts of computation now cheaper than write task-specific program.<br> \n",
    "\n",
    "#### E.g.s of tasks<br>\n",
    "1. Recpgmozomg patterms:<br>\n",
    "    (1). objects in real scenes<br>\n",
    "    (2). Facial identities/expressions<br>\n",
    "    (3). Spoken words<br>\n",
    "2. Recpmgmozomg anomalies:<br>\n",
    "    (1). Unusual sequences of credit crd<br>\n",
    "    (2). Unusual patterns of sensor reading in nuclear power plant<br>\n",
    "3. Predition:<br>\n",
    "    (1). Future Stock prices or currency exchange rates<br>\n",
    "\n",
    "#### Standard e.g. of ML\n",
    "1. lot of geneics done on fruit files<br>\n",
    "    (1). convenient b/c breed fast<br>\n",
    "    (2). we already know a lot<br>\n",
    "2. MNIST <br>\n",
    "    (1). publicly available and can learn fast in a moderate-sized neural net<br>\n",
    "    (2). know various ML methods do on MNIST<br>\n",
    "3. Standard task, MNIST<br>\n",
    "The fllowing picture is example image in the MNIST, hard to say which is \"2\".<br>\n",
    "![img1](imgs/img1.jpg)<br>\n",
    "\n",
    "#### ImageNet Task\n",
    "1. 1000 diff. obejct classes in 1.3 million high-resolution training images.<br>\n",
    "    (1). 2010 25% error for top 5 choices<br>\n",
    "2. Good test whether deep neural networks work well for object reognition<br>\n",
    "    (2). Very deep NN: < 20% for top 5 choices, 2012<br>\n",
    "Some example from the NN:<br>\n",
    "![img2](imgs/img2.jpg)<br>\n",
    "![img3](imgs/img3.jpg)<br>\n",
    "![img4](imgs/img4.jpg)<br>\n",
    "\n",
    "#### Speech Recognition\n",
    "1. Stages:<br>\n",
    "    (1). Pre-processing: Convert sound wave to vector of acoustic coefficient. Extract new vector about every 10 mille second.<br>\n",
    "    (2). The acoustic model: Use a few adjacent vectors of acoustic coefficients to place bets on which part of which poneme is being spoken.<br>\n",
    "    (3). Decoding: Find the sequence of bets that does the best job of fitting the coustic data and fitting a model.<br>\n",
    "2. DNN now replacing the previous ML for the acoustic model.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are neural networks?\n",
    "\n",
    "#### Reason to study Neural computation\n",
    "1. Understand how brain actually works.<br>\n",
    "    (1). Computer simulations<br>\n",
    "2. Understand style of parallel computation inspired by neurons and their adaptive connections.<br>\n",
    "    (1). diff. from sequential computation<br>\n",
    "3. solve practical problem using novel learning algorithms inspired by the brain<br>\n",
    "\n",
    "#### Typical cortical neuron\n",
    "1. Gross physical structure:<br>\n",
    "    (1). One axon that branches<br>\n",
    "    (2). a dendritic tree that collects input from other neurons<br>\n",
    "2. Axons contact dendritic trees at synapses<br>\n",
    "3. Spike generation<br>\n",
    "    (1). axon hillock, generates outgoing spikes<br>\n",
    "![img5](imgs/img5.jpg)<br>\n",
    "\n",
    "#### How the brain works on one slide\n",
    "1. Each neuron receives inputs from other neurons<br>\n",
    "2. Effect of each input line on the neuron is controlled by a synaptic weight<br>\n",
    "3. Synaptic weights adapt so the whole network learns to perform useful computations<br>\n",
    "    (1). Recongnizing obejcts, understanding language, making plans, controlling the body<br>\n",
    "4. $10^{11}$ neurons each with about $10^4$ weights<br>\n",
    "\n",
    "#### Modularity and the brain\n",
    "1. Diff. bits of  the cortex do diff things.<br>\n",
    "2. Cortex looks pretty much the same all over\n",
    "3. cortex can turn into special purpose hardware in response to experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple models of neurons\n",
    "\n",
    "#### Idealized neurons\n",
    "1. Idealize:\n",
    "    (1). removes complecated details that not essential for understanding the main principles\n",
    "    (2). Allows us to apply mathematics and to make analogies to other, familiar systems\n",
    "    (3). understand, than easy to add complexity to make the model more faithful\n",
    "2. It is often worth understanding models that are known to be wrong\n",
    "\n",
    "#### Linear neurons\n",
    "1. Simple but computationally limited\n",
    "$$y = b + \\sum_{i}x_i w_i$$\n",
    "$y$ is output, $b$ is bias, $i$ is index over input connections, $x_i$ is the ith input, $w_i$ is the weight on ith input.<br>\n",
    "![img6](imgs/img6.jpg)<br>\n",
    "\n",
    "#### Binary threshold neurons\n",
    "1. Comput weighted sum of the inputs<br>\n",
    "2. send out fixed size spike of activity if exceeds a threshold<br>\n",
    "3. Each spike is like the truth value of a proposition and each neuron combines truth values to compute the trith value of another propostion.<br>\n",
    "![img7](imgs/img7.jpg)<br>\n",
    "4. 2 equivalent equations:<br>\n",
    "    (1). $$z = \\sum_i x_iw_i$$\n",
    "    $$\\text{y} = \\begin{cases} 1 & \\quad \\text{if}\\quad z\\geq\\theta \\\\  0 & \\quad \\text{otherwise}\\\\ \\end{cases}$$\n",
    "    (2).  $$z = b + \\sum_i x_iw_i$$\n",
    "    $$\\text{y} = \\begin{cases} 1 & \\quad \\text{if}\\quad z\\geq 0 \\\\  0 & \\quad \\text{otherwise}\\\\ \\end{cases}$$\n",
    "$$\\theta = -b$$\n",
    "\n",
    "#### Rectified Linear Neurons (Linear threshold neurons)\n",
    "1. Compute linear weighted sum of their inputs<br>\n",
    "2. Output is a non-linear function fo the total input.<br>\n",
    "$$z = b + \\sum_i x_iw_i$$\n",
    "    $$\\text{y} = \\begin{cases} z & \\quad \\text{if}\\quad z > 0 \\\\  0 & \\quad \\text{otherwise}\\\\ \\end{cases}$$\n",
    "![img8](imgs/img8.jpg)<br>\n",
    "\n",
    "#### Sigmoid neurons\n",
    "1. A real-values output, smooth and bounded function of their total input<br>\n",
    "    (1). logistic function<br>\n",
    "    (2). nice derivatives make learning easy<br>\n",
    "    $$x=b+\\sum_i x_iw_i$$\n",
    "    $$y = \\frac{1}{1+e^{-z}}$$\n",
    "![img9](imgs/img9.jpg)<br>   \n",
    "\n",
    "#### Stochastic binary neurons\n",
    "1. Same as logistic units<br>\n",
    "    (1). but treat the output of the logistic as probability of producing a spike in a short time window<br>\n",
    "2. similar trick for rectified linear units<br>\n",
    "    (1). Output is treated as the Poisson rate for spikes<br>\n",
    "$$z = b + \\sum_i x_iw_i$$\n",
    "$$p(s=1) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple example of learning\n",
    "\n",
    "#### Very simple way to recognize handwritten shapes\n",
    "1. 2 layers of neurons<br>\n",
    "    (1). top layer represent known shapes<br>\n",
    "    (2). bottom layer represent pixel intensities<br>\n",
    "2. A pixel gets to vote if it has ink on it<br>\n",
    "    (1). combination for diff. shapes<br>\n",
    "3. Shape gets the most votes win<br>\n",
    "![img10](imgs/img10.jpg)<br>   \n",
    "\n",
    "#### Display the weights\n",
    "Give each output unit its own \"map\" of the input image and display the weight coming from each pixel in the location of that pixel in the map.<br>\n",
    "Use a black or white blob with the area representing the magnitude of the weight and the color representing the sign.<br>\n",
    "![img11](imgs/img11.jpg)<br>\n",
    "\n",
    "#### Learn the weights\n",
    "Show the network an image and increment the weights from active pixels to the correct class<br>\n",
    "Then decrement the weights from active pixels to whatever class the network guesses.<br>\n",
    "The process for learning weight:<br>\n",
    "![img12](imgs/img12.jpg)<br>\n",
    "![img13](imgs/img13.jpg)<br>\n",
    "\n",
    "#### Simple learning algorithm is insufficient\n",
    "1. 2-layer network w/ single winner in the top layer $=$ a rigid template for each shape<br>\n",
    "    (1). Winner is the template that has the biggest overlap with the ink.<br>\n",
    "2. Hand-written digits vary are much too complicated to be captured by simple template mathces of whole shapes.<br>\n",
    "    (1). To capture all allowable variations of a digit, need learn the features that composed of.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three types of learning\n",
    "\n",
    "#### Types of learning task\n",
    "1. Supervised learning<br>\n",
    "    (1). Learn to predict an output given an input vector.<br>\n",
    "2. Reinforcement learning<br>\n",
    "    (1). Learn to select an action to maximize payoff<br>\n",
    "3. Unsupervised learning<br>\n",
    "    (1). Discover a good internal representation of the inout<br>\n",
    "    \n",
    "#### Two types of supervised learning\n",
    "1. Each traning consists<br>\n",
    "    (1). Input vector $x$<br>\n",
    "    (2). Target output $t$<br>\n",
    "2. Regression: output is a real # or whole vector of real #<br>\n",
    "    (1). Price of stock<br>\n",
    "    (2). Temperature<br>\n",
    "3. Classification: output is a class label<br>\n",
    "    (1). Choice b/w 1 and 0<br>\n",
    "    (2). Have multiple alternative labels<br>\n",
    "\n",
    "#### How supervised learning work\n",
    "1. Choosing a model-class: $y=f(x;W)$<br>\n",
    "    (1). A model-class, $f$, is a way of using some numerical parameters, $W$, to map each input vector, $x$, into a predicted output $y$.<br>\n",
    "2. Learning usually means adjusting the parameters to reduce the discrepancy between the target output, $t$, on each training case and the actual output, $y$, produced by the model.<br>\n",
    "    (1). For regression, $\\frac{1}{2}(y-t)^2$ is often a sensible measure of the discrepancy.<br>\n",
    "    (2). For classification there are other measures that are generally more sensible.<br>\n",
    "\n",
    "#### Reinforcement learning\n",
    "1. Output is an action or sequence of actions and the only supervisory signal is an occasional scalar rewrd.<br>\n",
    "    (1). Goal: maximize the expected sum of the future rewards.<br>\n",
    "    (2). Use a discount factor for delayed rewards, don't have to look too far into the future.<br>\n",
    "2. Reinforcement learning is difficult:<br>\n",
    "    (1). Rewards are typically delayed so hard to know there went wrong (or right).<br>\n",
    "    (2). Scalar reward does not supply much information.<br>\n",
    "3. not cover in this course<br>\n",
    "\n",
    "#### Unsupervised learning\n",
    "1. ignored by the ML for about 40 yrs.<br>\n",
    "    (1). some widely used def. of ML excluded it.<br>\n",
    "    (2). Many think clustering was the only form of unsupervised learning<br>\n",
    "2. hard to sat the aim<br>\n",
    "    (1). Major aim: create internal representation of the input, useful for subsequent supervised or reinforcement learning<br>\n",
    "    (2). Can compute the distance to a surface using the disparity between 2 images. But not to compute disparities by stubbing toe thousands of times.<br>\n",
    "    \n",
    "#### Other goals for UL\n",
    "1. provides a compact, low-dimensional representation of the input<br>\n",
    "2. provides an economical high-dimensional representation of the input in terms of learned features<br>\n",
    "3. finds sensible clusters in input.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
